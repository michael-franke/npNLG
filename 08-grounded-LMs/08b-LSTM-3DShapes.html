
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Sheet 8.1: An LSTM-based image captioner for the annotated 3D-Shapes data set &#8212; Neural Pragmatic Natural Language Generation</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Sheet 8.2: Using 🤗’s pretrained models for image captioning" href="08c-NIC-pretrained.html" />
    <link rel="prev" title="Grounded Language Models" href="08a-grounded-LMs.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/npNLG-logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Neural Pragmatic Natural Language Generation</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../000-intro.html">
                    Pragmatic Natural Language Generation with Neural Language Models
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  00 organization
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../00-organization/00a-overview.html">
   Course overview
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../00-organization/00b-practicalities.html">
   Practicalities
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../00-organization/00c-schedule.html">
   Schedule
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../00-organization/00d-slides.html">
   Slides from session 1
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  01 Probabilistic pragmatics
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../01-prob-prag/01a-prob-prag.html">
   Probabilistic pragmatics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../01-prob-prag/01b-RSA-vanilla.html">
   Sheet 1.1: Vanilla RSA
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../01-prob-prag/01c-RSA-politeness.html">
   Sheet 1.2: RSA with politeness
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  02 PyTorch Basics
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../02-pytorch/02a-pytorch-basics.html">
   Basics of PyTorch
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../02-pytorch/02b-pytorch-intro.html">
   Sheet 2.1: PyTorch essentials
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../02-pytorch/02c-MLE.html">
   Sheet 2.2: ML-estimation
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  03 Optimization
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../03-optimization/03a-optimization.html">
   Optimization in PyTorch
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../03-optimization/03b-gradient-descent.html">
   Sheet 3.1: Gradient descent by hand
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../03-optimization/03c-RSA-pytorch.html">
   Sheet 3.2: Optimizing an RSA model
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  04 ANNs in PyTorch
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../04-ANNs/04a-ANNs.html">
   Artificial neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../04-ANNs/04b-algebra.html">
   Linear algebra
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../04-ANNs/04c-MLP-custom.html">
   Sheet 4.1: Non-linear regression (custom MLP)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../04-ANNs/04d-MLP-pytorch.html">
   Sheet 4.2: Non-linear regression (MLP w/ PyTorch modules)
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  05 RNNs in PyTorch
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../05-RNNs/05a-RNNs.html">
   Recurrent neural networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../05-RNNs/05b-char-level-RNN.html">
   Sheet 5.1: Character-level sequence modeling w/ RNNs
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  06 LSTMs in PyTorch
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../06-LSTMs/06a-LSTMs.html">
   Long-Short Term Memory (LSTM) models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../06-LSTMs/06b-LSTM-minimal-forward-pass.html">
   Sheet 6.1: Anatomy of a single LSTM forward pass
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../06-LSTMs/06c-char-level-LSTM.html">
   Sheet 6.2: Character-level sequence modeling w/ LSTMs
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../06-LSTMs/06d-decoding-GPT2.html">
   Sheet 6.3: Decoding strategies
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  07 Large language models
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../07-LLMs/07a-LLMs.html">
   Large Language Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../07-LLMs/07b-pretrained-LLMs.html">
   Sheet 7.1: Using pretrained LLMs w/ the ‘transformers’ package
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  08 Grounded language models
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="08a-grounded-LMs.html">
   Grounded Language Models
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Sheet 8.1: An LSTM-based image captioner for the annotated 3D-Shapes data set
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="08c-NIC-pretrained.html">
   Sheet 8.2: Using 🤗’s pretrained models for image captioning
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  09 neural-pragmatic NLG
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../09-np-NLG/09a-np-NLG.html">
   Neural pragmatic Natural Language Generation
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/michael-franke/npNLG/main?urlpath=tree/neural_pragmatic_nlg/08-grounded-LMs/08b-LSTM-3DShapes.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
      <li>
        <a href="https://colab.research.google.com/github/michael-franke/npNLG/blob/main/neural_pragmatic_nlg/08-grounded-LMs/08b-LSTM-3DShapes.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Colab"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="headerbtn__text-container">Colab</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/michael-franke/npNLG"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/michael-franke/npNLG/issues/new?title=Issue%20on%20page%20%2F08-grounded-LMs/08b-LSTM-3DShapes.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/08-grounded-LMs/08b-LSTM-3DShapes.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#credits-and-origin-of-material">
   Credits and origin of material
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#necessary-files">
   Necessary files
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#packages">
   Packages
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-a3ds-data-set">
   The A3DS data set
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-dataset-object">
     The ’Dataset’ object
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#creating-a-dataloader">
     Creating a ’DataLoader’
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-pre-trained-lstm-nic">
   The (pre-trained) LSTM NIC
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Sheet 8.1: An LSTM-based image captioner for the annotated 3D-Shapes data set</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#credits-and-origin-of-material">
   Credits and origin of material
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#necessary-files">
   Necessary files
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#packages">
   Packages
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-a3ds-data-set">
   The A3DS data set
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-dataset-object">
     The ’Dataset’ object
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#creating-a-dataloader">
     Creating a ’DataLoader’
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-pre-trained-lstm-nic">
   The (pre-trained) LSTM NIC
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="sheet-8-1-an-lstm-based-image-captioner-for-the-annotated-3d-shapes-data-set">
<h1>Sheet 8.1: An LSTM-based image captioner for the annotated 3D-Shapes data set<a class="headerlink" href="#sheet-8-1-an-lstm-based-image-captioner-for-the-annotated-3d-shapes-data-set" title="Permalink to this headline">#</a></h1>
<p><strong>Author:</strong> Michael Franke &amp; Polina Tsvilodub</p>
<p>In order to see how a custom-built neural image captioning (NIC) system can be implemented and used for inference, we look here at a relatively simple LSTM-based image captioner.
To shortcut the time it takes for training this model, we use downloadable weights from a trained version of this model.
The goal is to see the architecture in full detail and to get a feeling for the way in which images and text data is handled during inference.
Most importantly, however, we will want to get a feeling for how good the model’s predictions are, at least intuitively.</p>
<p>This tutorial uses a synthetic data set of annotations for the <a class="reference external" href="https://github.com/deepmind/3d-shapes">3D Shapes data set</a>, which we will refer to as “annotated 3D Shapes data set” or “A3DS” for short.
Other image-captioning data sets (like MS-Coco) contain heterogeneous pictures and often only a small number of captions per picture, making it less clear whether it is the NIC’s fault or the potentially poor quality of the data set that causes generations to be intuitively inadequate (garbled, untrue, over- or underspecified …).
In contrast, using the A3DS data set, makes it easier to judge, on intuitive grounds, whether generated captions are any good.</p>
<section id="credits-and-origin-of-material">
<h2>Credits and origin of material<a class="headerlink" href="#credits-and-origin-of-material" title="Permalink to this headline">#</a></h2>
<p>The material presented in this tutorial is in large part based on work conducted by Polina Tsvilodub for her 2022 MSc thesis “Language Drift of Multi-Agent Communication Systems in Reference Games”.
In particular, the annotated 3D Shapes data set and the LSTM-based architecture stems from this thesis.</p>
</section>
<section id="necessary-files">
<h2>Necessary files<a class="headerlink" href="#necessary-files" title="Permalink to this headline">#</a></h2>
<p>You need additional files to run the code in this notebook.
If you are on CoLab use these commands to install.
(Check if the files are installed in the right directory (’A3DS’) after unzipping).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># !wget https://github.com/michael-franke/npNLG/raw/main/neural_pragmatic_nlg/data/A3DS/A3DS.zip</span>
<span class="c1"># !unzip A3DS.zip</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="packages">
<h2>Packages<a class="headerlink" href="#packages" title="Permalink to this headline">#</a></h2>
<p>On top of the usual suspects, we will use the ’Image’ and ’torchvision’ package to process image data.
We need package ’pickle’ to load image data and pre-trained model weights.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">##################################################</span>
<span class="c1">## import packages</span>
<span class="c1">##################################################</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">Dataset</span>
<span class="kn">from</span> <span class="nn">torchtext.data</span> <span class="kn">import</span> <span class="n">get_tokenizer</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">transforms</span>
<span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">from</span> <span class="nn">random</span> <span class="kn">import</span> <span class="n">shuffle</span>
<span class="c1"># from tqdm import tqdm</span>
<span class="kn">import</span> <span class="nn">pickle</span>
<span class="c1"># import 5py</span>

<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s1">&#39;ignore&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="the-a3ds-data-set">
<h2>The A3DS data set<a class="headerlink" href="#the-a3ds-data-set" title="Permalink to this headline">#</a></h2>
<p>The following code gives us PyTorch ’Dataset’ and ’DataLoader’ objects, with which to handle a 1k-subset of images and annotations from the A3DS data set.</p>
<section id="the-dataset-object">
<h3>The ’Dataset’ object<a class="headerlink" href="#the-dataset-object" title="Permalink to this headline">#</a></h3>
<p>Here is the definition of the ’Dataset’ object.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">A3DS</span><span class="p">(</span><span class="n">Dataset</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Dataset class for loading the dataset of images and captions from the 3dshapes dataset.</span>

<span class="sd">    Arguments:</span>
<span class="sd">    ---------</span>
<span class="sd">    num_labels: int</span>
<span class="sd">        Number of distinct captions to sample for each image. Relevant for using the dataloader for training models.</span>
<span class="sd">    labels_type: str</span>
<span class="sd">        &quot;long&quot; or &quot;short&quot;. Indicates whether long or short captions should be used.</span>
<span class="sd">    run_inference: bool</span>
<span class="sd">        Flag indicating whether this dataset will be used for performing inference with a trained image captioner.</span>
<span class="sd">    batch_size: int</span>
<span class="sd">        Batch size. Has to be 1 in order to save the example image-caption pairs.</span>
<span class="sd">    vocab_file: str</span>
<span class="sd">        Name of vocab file.</span>
<span class="sd">    start_token: str</span>
<span class="sd">        Start token.</span>
<span class="sd">    end_token: str</span>
<span class="sd">        End token.</span>
<span class="sd">    unk_token: str</span>
<span class="sd">        Token to be used when encoding unknown tokens.</span>
<span class="sd">    pad_token: str</span>
<span class="sd">        Pad token to be used for padding captions tp max_sequence_length.</span>
<span class="sd">    max_sequence_length: int</span>
<span class="sd">        Length to which all captions are padded / truncated.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">path</span><span class="o">=</span><span class="s2">&quot;A3DS&quot;</span><span class="p">,</span>
            <span class="n">num_labels</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="c1"># number of ground truth labels to retrieve per image</span>
            <span class="n">labels_type</span><span class="o">=</span><span class="s2">&quot;long&quot;</span><span class="p">,</span> <span class="c1"># alternative: short</span>
            <span class="n">run_inference</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="c1"># depending on this flag, check presence of model weights</span>
            <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">vocab_file</span><span class="o">=</span><span class="s2">&quot;vocab.pkl&quot;</span><span class="p">,</span>
            <span class="n">start_token</span><span class="o">=</span><span class="s2">&quot;START&quot;</span><span class="p">,</span>  <span class="c1"># might be unnecessary since vocab file is fixed anyways</span>
            <span class="n">end_token</span><span class="o">=</span><span class="s2">&quot;END&quot;</span><span class="p">,</span>
            <span class="n">unk_token</span><span class="o">=</span><span class="s2">&quot;UNK&quot;</span><span class="p">,</span>
            <span class="n">pad_token</span><span class="o">=</span><span class="s2">&quot;PAD&quot;</span><span class="p">,</span>
            <span class="n">max_sequence_length</span><span class="o">=</span><span class="mi">26</span><span class="p">,</span> <span class="c1"># important for padding length</span>
        <span class="p">):</span>

        <span class="c1"># check vocab file exists</span>
        <span class="k">assert</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">vocab_file</span><span class="p">)),</span> <span class="s2">&quot;Make sure the vocab file exists in the directory passed to the dataloader (see README)&quot;</span>

        <span class="c1"># check if image file exists</span>
        <span class="k">assert</span> <span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="s2">&quot;sandbox_3Dshapes_1000.pkl&quot;</span><span class="p">))</span> <span class="ow">and</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="s2">&quot;sandbox_3Dshapes_resnet50_features_1000.pt&quot;</span><span class="p">)),</span> <span class="s2">&quot;Make sure the sandbox dataset exists in the directory passed to the dataloader (see README)&quot;</span>

        <span class="k">if</span> <span class="n">labels_type</span> <span class="o">==</span> <span class="s2">&quot;long&quot;</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">num_labels</span> <span class="o">&lt;=</span> <span class="mi">20</span><span class="p">,</span> <span class="s2">&quot;Maximally 20 distinct image-long caption pairs can be created for one image&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">assert</span> <span class="n">num_labels</span> <span class="o">&lt;=</span> <span class="mi">27</span><span class="p">,</span> <span class="s2">&quot;Maximally 27 distinct image-short caption pairs can be created for one image&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="n">vocab_file</span><span class="p">),</span> <span class="s2">&quot;rb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">vf</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">vocab</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">vf</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">max_sequence_length</span> <span class="o">=</span> <span class="n">max_sequence_length</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">start_token</span> <span class="o">=</span> <span class="n">start_token</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">end_token</span> <span class="o">=</span> <span class="n">end_token</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">unk_token</span> <span class="o">=</span> <span class="n">unk_token</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pad_token</span> <span class="o">=</span> <span class="n">pad_token</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">get_tokenizer</span><span class="p">(</span><span class="s2">&quot;basic_english&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">embedded_imgs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="s2">&quot;sandbox_3Dshapes_resnet50_features_1000.pt&quot;</span><span class="p">))</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="s2">&quot;sandbox_3Dshapes_1000.pkl&quot;</span><span class="p">),</span> <span class="s2">&quot;rb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">sandbox_file</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">images</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sandbox_file</span><span class="p">[</span><span class="s2">&quot;images&quot;</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">numeric_labels</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sandbox_file</span><span class="p">[</span><span class="s2">&quot;labels_numeric&quot;</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">labels_long</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sandbox_file</span><span class="p">[</span><span class="s2">&quot;labels_long&quot;</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">labels_short</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">sandbox_file</span><span class="p">[</span><span class="s2">&quot;labels_short&quot;</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">labels_type</span> <span class="o">==</span> <span class="s2">&quot;long&quot;</span><span class="p">:</span>
            <span class="n">labels_ids_flat</span> <span class="o">=</span> <span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">labels_long</span><span class="p">[</span><span class="mi">0</span><span class="p">])),</span> <span class="n">num_labels</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">images</span><span class="p">))]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">labels_flat</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">labels_long</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">l</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">sublst</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">labels_ids_flat</span><span class="p">)</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">sublst</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">img_ids_flat</span> <span class="o">=</span> <span class="p">[</span><span class="nb">id</span> <span class="k">for</span> <span class="nb">id</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">images</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_labels</span><span class="p">)]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">labels_ids_flat</span> <span class="o">=</span> <span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">labels_short</span><span class="p">[</span><span class="mi">0</span><span class="p">])),</span> <span class="n">num_labels</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">images</span><span class="p">))]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">labels_flat</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">labels_short</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">l</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">sublst</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">labels_ids_flat</span><span class="p">)</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">sublst</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">img_ids_flat</span> <span class="o">=</span> <span class="p">[</span><span class="nb">id</span> <span class="k">for</span> <span class="nb">id</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">images</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_labels</span><span class="p">)]</span>

        <span class="c1"># print(&quot;len labels ids flat &quot;, len(labels_ids_flat))</span>
        <span class="c1"># print(&quot;len labels flat &quot;, len(self.labels_flat), self.labels_flat[:5])</span>
        <span class="c1"># print(&quot;len image ids flat &quot;, len(self.img_ids_flat), self.img_ids_flat[:5])</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns length of dataset.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">img_ids_flat</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Iterator over the dataset.</span>

<span class="sd">        Arguments:</span>
<span class="sd">        ---------</span>
<span class="sd">        idx: int</span>
<span class="sd">            Index for accessing the flat image-caption pairs.</span>

<span class="sd">        Returns:</span>
<span class="sd">        -------</span>
<span class="sd">        target_img: np.ndarray (64,64,3)</span>
<span class="sd">            Original image.</span>
<span class="sd">        target_features: torch.Tensor(2048,)</span>
<span class="sd">            ResNet features of the image.</span>
<span class="sd">        target_lbl: str</span>
<span class="sd">            String caption.</span>
<span class="sd">        numeric_lbl: np.ndarray (6,)</span>
<span class="sd">            Original numeric image annotation.</span>
<span class="sd">        target_caption: torch.Tensor(batch_size, 25)</span>
<span class="sd">            Encoded caption.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># access raw image corresponding to the index in the entire dataset</span>
        <span class="n">target_img</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">images</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">img_ids_flat</span><span class="p">[</span><span class="n">idx</span><span class="p">]]</span>
        <span class="c1"># access caption</span>
        <span class="n">target_lbl</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">labels_flat</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="c1"># access original numeric annotation of the image</span>
        <span class="n">numeric_lbl</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">numeric_labels</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">img_ids_flat</span><span class="p">[</span><span class="n">idx</span><span class="p">]]</span>
        <span class="c1"># cast type</span>
        <span class="n">target_img</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">target_img</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;uint8&#39;</span><span class="p">)</span>
        <span class="c1"># retrieve ResNet features, accessed through original image ID</span>
        <span class="n">target_features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedded_imgs</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">img_ids_flat</span><span class="p">[</span><span class="n">idx</span><span class="p">]]</span>
        <span class="c1"># tokenize label</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">target_lbl</span><span class="p">)</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;-&quot;</span><span class="p">,</span> <span class="s2">&quot; &quot;</span><span class="p">))</span>
        <span class="c1"># Convert caption to tensor of word ids, append start and end tokens.</span>
        <span class="n">target_caption</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenize_caption</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
        <span class="c1"># convert to tensor</span>
        <span class="n">target_caption</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">target_caption</span><span class="p">)</span><span class="o">.</span><span class="n">long</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">target_img</span><span class="p">,</span> <span class="n">target_features</span><span class="p">,</span> <span class="n">target_lbl</span><span class="p">,</span> <span class="n">numeric_lbl</span><span class="p">,</span> <span class="n">target_caption</span>

    <span class="k">def</span> <span class="nf">tokenize_caption</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Helper for converting list of tokens into list of token IDs.</span>
<span class="sd">        Expects tokenized caption as input.</span>

<span class="sd">        Arguments:</span>
<span class="sd">        --------</span>
<span class="sd">        label: list</span>
<span class="sd">            Tokenized caption.</span>

<span class="sd">        Returns:</span>
<span class="sd">        -------</span>
<span class="sd">        tokens: list</span>
<span class="sd">            List of token IDs, prepended with start, end, padded to max length.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">label</span> <span class="o">=</span> <span class="n">label</span><span class="p">[:(</span><span class="bp">self</span><span class="o">.</span><span class="n">max_sequence_length</span><span class="o">-</span><span class="mi">2</span><span class="p">)]</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="s2">&quot;word2idx&quot;</span><span class="p">][</span><span class="bp">self</span><span class="o">.</span><span class="n">start_token</span><span class="p">]]</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">label</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="s2">&quot;word2idx&quot;</span><span class="p">][</span><span class="n">t</span><span class="p">])</span>
            <span class="k">except</span><span class="p">:</span>
                <span class="n">tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="s2">&quot;word2idx&quot;</span><span class="p">][</span><span class="bp">self</span><span class="o">.</span><span class="n">unk_token</span><span class="p">])</span>
        <span class="n">tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="s2">&quot;word2idx&quot;</span><span class="p">][</span><span class="bp">self</span><span class="o">.</span><span class="n">end_token</span><span class="p">])</span>
        <span class="c1"># pad</span>
        <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_sequence_length</span><span class="p">:</span>
            <span class="n">tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="s2">&quot;word2idx&quot;</span><span class="p">][</span><span class="bp">self</span><span class="o">.</span><span class="n">pad_token</span><span class="p">])</span>

        <span class="k">return</span> <span class="n">tokens</span>

    <span class="k">def</span> <span class="nf">get_labels_for_image</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">id</span><span class="p">,</span> <span class="n">caption_type</span><span class="o">=</span><span class="s2">&quot;long&quot;</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Helper for getting all annotations for a given image id.</span>

<span class="sd">        Arguments:</span>
<span class="sd">        ---------</span>
<span class="sd">        id: int</span>
<span class="sd">            Index of image caption pair containing the image</span>
<span class="sd">            for which the full list of captions should be returned.</span>
<span class="sd">        caption_type: str</span>
<span class="sd">            &quot;long&quot; or &quot;short&quot;. Indicates type of captions to provide.</span>

<span class="sd">        Returns:</span>
<span class="sd">        -------</span>
<span class="sd">            List of all captions for given image.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">caption_type</span> <span class="o">==</span> <span class="s2">&quot;long&quot;</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">labels_long</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">img_ids_flat</span><span class="p">[</span><span class="nb">id</span><span class="p">]]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">labels_short</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">img_ids_flat</span><span class="p">[</span><span class="nb">id</span><span class="p">]]</span>
</pre></div>
</div>
</div>
</div>
<p>Lets instantiate the ’Dataset’ object and explore the structure of the A3DS data.
Notice that there are a 1000 items in this subset of the A3DS data set.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">A3DS_dataset</span> <span class="o">=</span> <span class="n">A3DS</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">A3DS_dataset</span><span class="o">.</span><span class="fm">__len__</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>1000
</pre></div>
</div>
</div>
</div>
<p>Let’s get a single item by some ID, here taking the first item.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">itemID</span><span class="o">=</span><span class="mi">0</span>
<span class="n">image</span><span class="p">,</span> <span class="n">target_features</span><span class="p">,</span> <span class="n">caption_text</span><span class="p">,</span> <span class="n">numeric_lbl</span><span class="p">,</span> <span class="n">caption_indx</span> <span class="o">=</span> <span class="n">A3DS_dataset</span><span class="o">.</span><span class="fm">__getitem__</span><span class="p">(</span><span class="n">itemID</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Each item is a tuple with 5 pieces of information.
For our purposes, the most important ones are in slot 0 (the image information) and in slot 2 (the caption as a text).</p>
<p>Let’s have a look at the image, which is stored as a tensor.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># picture</span>
<span class="nb">print</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>

<span class="c1"># plot image</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">image</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>#+begin_example
[[[153 226 249]
  [153 226 249]
  [153 226 249]
  ...
  [153 226 249]
  [153 226 249]
  [153 226 249]]

 [[153 226 249]
  [153 226 249]
  [153 226 249]
  ...
  [153 226 249]
  [153 226 249]
  [153 226 249]]

 [[153 226 249]
  [153 226 249]
  [153 226 249]
  ...
  [153 226 249]
  [153 226 249]
  [153 226 249]]

 ...

 [[254   0   0]
  [254   0   0]
  [253   0   0]
  ...
  [214   0   0]
  [216   0   0]
  [219   0   0]]

 [[251   0   0]
  [246   0   0]
  [250   0   0]
  ...
  [220   0   0]
  [215   0   0]
  [212   0   0]]

 [[255   0   0]
  [248   0   0]
  [243   0   0]
  ...
  [219   0   0]
  [219   0   0]
  [217   0   0]]]
#+end_example
</pre></div>
</div>
<img alt="../_images/08b-LSTM-3DShapes_20_1.png" src="../_images/08b-LSTM-3DShapes_20_1.png" />
</div>
</div>
<p>And here is a caption that goes with this picture.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># ground-truth caption</span>
<span class="nb">print</span><span class="p">(</span><span class="n">caption_text</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>the picture shows a small orange cylinder on red floor in the left corner in front of a purple wall
</pre></div>
</div>
</div>
</div>
<p>There are actually long and short captions for each image.
We have created an instance of the data set with one random long caption per image.
We can inspect the full list of short captions like so:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Retrieve all short-captions for the image ID:</span>
<span class="n">all_short_caps</span> <span class="o">=</span> <span class="n">A3DS_dataset</span><span class="o">.</span><span class="n">get_labels_for_image</span><span class="p">(</span><span class="n">itemID</span><span class="p">,</span> <span class="n">caption_type</span><span class="o">=</span><span class="s1">&#39;short&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">all_short_caps</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>#+begin_example
there is a small cylinder
there is a orange cylinder
there is a cylinder in the left corner
there is a cylinder in front of a purple wall
there is a cylinder on red floor
there is a small cylinder in the left corner
there is a small cylinder in front of a purple wall
there is a small cylinder on red floor
there is a orange cylinder in the left corner
there is a orange cylinder in front of a purple wall
there is a orange cylinder on red floor
a small cylinder
a orange cylinder
a cylinder in the left corner
a cylinder in front of a purple wall
a cylinder on red floor
a small cylinder in the left corner
a small cylinder in front of a purple wall
a small cylinder on red floor
a orange cylinder in the left corner
a orange cylinder in front of a purple wall
a orange cylinder on red floor
the cylinder is in the left corner
the cylinder is in front of a purple wall
the cylinder is on red floor
the cylinder is small
the cylinder is orange
#+end_example
</pre></div>
</div>
</div>
</div>
<p>And similarly for the long captions.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Retrieve all long-captions for the image ID:</span>

<span class="n">all_long_caps</span> <span class="o">=</span> <span class="n">A3DS_dataset</span><span class="o">.</span><span class="n">get_labels_for_image</span><span class="p">(</span><span class="n">itemID</span><span class="p">,</span> <span class="n">caption_type</span><span class="o">=</span><span class="s1">&#39;long&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">all_long_caps</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">c</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>#+begin_example
a small orange cylinder in the left corner in front of a purple wall on red floor
a small orange cylinder in the left corner on red floor in front of a purple wall
a small orange cylinder on red floor in the left corner in front of a purple wall
a small orange cylinder on red floor in front of a purple wall in the left corner
the picture shows a small orange cylinder in the left corner in front of a purple wall on red floor
the picture shows a small orange cylinder in the left corner on red floor in front of a purple wall
the picture shows a small orange cylinder on red floor in the left corner in front of a purple wall
the picture shows a small orange cylinder on red floor in front of a purple wall in the left corner
a small orange cylinder located in the left corner in front of a purple wall on red floor
a small orange cylinder located in the left corner on red floor in front of a purple
a small orange cylinder located on red floor in the left corner in front of a purple wall
a small orange cylinder located on red floor in front of a purple wall in the left corner
the small cylinder in the left corner in front of a purple wall on red floor is orange
the small cylinder in the left corner on red floor in front of a purple wall is orange
the small cylinder on red floor in the left corner in front of a purple wall is orange
the small cylinder on red floor in front of a purple wall in the left corner is orange
the orange cylinder in the left corner in front of a purple wall on red floor is small
the orange cylinder in the left corner on red floor in front of a purple wall is small
the orange cylinder on red floor in the left corner in front of a purple wall is small
the orange cylinder on red floor in front of a purple wall in the left corner is small
#+end_example
</pre></div>
</div>
</div>
</div>
<p>Finally, let’s also have a look at the vocabulary for this A3DS data set:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">vocab</span> <span class="o">=</span> <span class="n">A3DS_dataset</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="s2">&quot;word2idx&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;VOCAB: &quot;</span><span class="p">,</span> <span class="n">vocab</span><span class="p">)</span>

<span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;VOCAB SIZE: &quot;</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>VOCAB:  dict_keys([&#39;START&#39;, &#39;END&#39;, &#39;UNK&#39;, &#39;PAD&#39;, &#39;a&#39;, &#39;tiny&#39;, &#39;red&#39;, &#39;block&#39;, &#39;in&#39;, &#39;the&#39;, &#39;right&#39;, &#39;corner&#39;, &#39;front&#39;, &#39;of&#39;, &#39;wall&#39;, &#39;on&#39;, &#39;floor&#39;, &#39;picture&#39;, &#39;shows&#39;, &#39;standing&#39;, &#39;is&#39;, &#39;close&#39;, &#39;to&#39;, &#39;side&#39;, &#39;near&#39;, &#39;middle&#39;, &#39;nearly&#39;, &#39;left&#39;, &#39;cylinder&#39;, &#39;ball&#39;, &#39;pill&#39;, &#39;small&#39;, &#39;medium&#39;, &#39;sized&#39;, &#39;big&#39;, &#39;large&#39;, &#39;huge&#39;, &#39;giant&#39;, &#39;orange&#39;, &#39;yellow&#39;, &#39;light&#39;, &#39;green&#39;, &#39;dark&#39;, &#39;cyan&#39;, &#39;blue&#39;, &#39;purple&#39;, &#39;pink&#39;])
VOCAB SIZE:  47
</pre></div>
</div>
</div>
</div>
<p>We see that this vocabulary is actually pretty small.</p>
</section>
<section id="creating-a-dataloader">
<h3>Creating a ’DataLoader’<a class="headerlink" href="#creating-a-dataloader" title="Permalink to this headline">#</a></h3>
<p>Let’s create a ’DataLoader’ for batches of a specified size, using a random shuffle of the data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">A3DS_data_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
    <span class="n">dataset</span>    <span class="o">=</span> <span class="n">A3DS_dataset</span><span class="p">,</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span><span class="p">,</span>
    <span class="n">shuffle</span>    <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="the-pre-trained-lstm-nic">
<h2>The (pre-trained) LSTM NIC<a class="headerlink" href="#the-pre-trained-lstm-nic" title="Permalink to this headline">#</a></h2>
<p>Definition of the LSTM-based neural image captioner as an instance of PyTorch’s ’nn.Module’:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">DecoderRNN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embed_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">visual_embed_size</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialize the language module consisting of a one-layer LSTM and</span>
<span class="sd">        trainable embeddings. The image embeddings (both target and distractor!)</span>
<span class="sd">        are used as additional context at every step of the training</span>
<span class="sd">        (prepended to each word embedding).</span>

<span class="sd">        Args:</span>
<span class="sd">        -----</span>
<span class="sd">            embed_size: int</span>
<span class="sd">                Dimensionality of trainable embeddings.</span>
<span class="sd">            hidden_size: int</span>
<span class="sd">                Hidden/ cell state dimensionality of the LSTM.</span>
<span class="sd">            vocab_size: int</span>
<span class="sd">                Length of vocabulary.</span>
<span class="sd">            visual_embed_size: int</span>
<span class="sd">                Dimensionality of each image embedding to be appended at each time step as additional context.</span>
<span class="sd">            batch_size: int</span>
<span class="sd">                Batch size.</span>
<span class="sd">            num_layers: int</span>
<span class="sd">                Number of LSTM layers.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DecoderRNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span> <span class="o">=</span> <span class="n">num_layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embed_size</span><span class="o">=</span> <span class="n">embed_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vocabulary_size</span> <span class="o">=</span> <span class="n">vocab_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">visual_embed_size</span> <span class="o">=</span> <span class="n">visual_embed_size</span>
        <span class="c1"># embedding layer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embed</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocabulary_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_size</span><span class="p">)</span>
        <span class="c1"># layer projecting ResNet features of a single image to desired size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">project</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2048</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">visual_embed_size</span><span class="p">)</span>

      <span class="c1"># LSTM takes as input the word embedding with prepended embeddings of the two images at each time step</span>
        <span class="c1"># note that the batch dimension comes first</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embed_size</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">visual_embed_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_layers</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="c1"># transforming last lstm hidden state to scores over vocabulary</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocabulary_size</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
        <span class="c1"># initial hidden state of the lstm</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_hidden</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span><span class="p">)</span>

        <span class="c1"># initialization of the layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embed</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="o">.</span><span class="n">bias</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">init_hidden</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>

<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        At the start of training, we need to initialize a hidden state;</span>
<span class="sd">        Defines a hidden state with all zeroes</span>
<span class="sd">        The axes are (num_layers, batch_size, hidden_size)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># if torch.backends.mps.is_available():</span>
        <span class="c1">#     device = torch.device(&quot;mps&quot;)</span>
        <span class="c1"># elif torch.cuda.is_available():</span>
        <span class="c1">#     device = torch.device(&quot;cuda&quot;)</span>
        <span class="c1"># else:</span>
        <span class="c1">#     device = torch.device(&quot;cpu&quot;)</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span>

        <span class="k">return</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">),</span> \
                <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">captions</span><span class="p">,</span> <span class="n">prev_hidden</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Perform forward step through the LSTM.</span>

<span class="sd">        Args:</span>
<span class="sd">        -----</span>
<span class="sd">            features: torch.tensor(batch_size, 2, embed_size)</span>
<span class="sd">                Embeddings of images, target and distractor concatenated in this order.</span>
<span class="sd">            captions: torch.tensor(batch_size, caption_length)</span>
<span class="sd">                Lists of indices representing tokens of each caption.</span>
<span class="sd">            prev_hidden: (torch.tensor(num_layers, batch_size, hidden_size), torch.tensor(num_layers, batch_size, hidden_size))</span>
<span class="sd">                Tuple containing previous hidden and cell states of the LSTM.</span>
<span class="sd">        Returns:</span>
<span class="sd">        ------</span>
<span class="sd">            outputs: torch.tensor(batch_size, caption_length, embedding_dim)</span>
<span class="sd">                Scores over vocabulary for each token in each caption.</span>
<span class="sd">            hidden_state: (torch.tensor(num_layers, batch_size, hidden_size), torch.tensor(num_layers, batch_size, hidden_size))</span>
<span class="sd">                Tuple containing new hidden and cell state of the LSTM.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># features of shape (batch_size, 2, 2048)</span>
        <span class="n">image_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">project</span><span class="p">(</span><span class="n">features</span><span class="p">)</span> <span class="c1"># image_emb should have shape (batch_size, 2, 512)</span>
        <span class="c1"># concatenate target and distractor embeddings</span>
        <span class="n">img_features</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">image_emb</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:],</span> <span class="n">image_emb</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:]),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed</span><span class="p">(</span><span class="n">captions</span><span class="p">)</span>
        <span class="c1"># repeat image features such that they can be prepended to each token</span>
        <span class="n">img_features_reps</span> <span class="o">=</span> <span class="n">img_features</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
        <span class="c1"># PREpend the feature embedding as additional context as first token, assume there is no END token</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">img_features_reps</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">out</span><span class="p">,</span> <span class="n">hidden_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">prev_hidden</span><span class="p">)</span>
        <span class="c1"># project LSTM predictions on to vocab</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">out</span><span class="p">)</span> <span class="c1"># prediction shape is (batch_size, max_sequence_length, vocab_size)</span>
        <span class="c1"># print(&quot;outputs shape in forward &quot;, outputs.shape)</span>
        <span class="k">return</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">hidden_state</span>

    <span class="k">def</span> <span class="nf">log_prob_helper</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">values</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Helper function for scoring the sampled token,</span>
<span class="sd">        because it is not implemented for MPS yet.</span>
<span class="sd">        Just duplicates source code from PyTorch.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">values</span> <span class="o">=</span> <span class="n">values</span><span class="o">.</span><span class="n">long</span><span class="p">()</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">values</span><span class="p">,</span> <span class="n">log_pmf</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">broadcast_tensors</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">logits</span><span class="p">)</span>
        <span class="n">values</span> <span class="o">=</span> <span class="n">values</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">log_pmf</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">values</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">max_sequence_length</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Function for sampling a caption during functional (reference game) training.</span>
<span class="sd">        Implements greedy sampling. Sampling stops when END token is sampled or when max_sequence_length is reached.</span>
<span class="sd">        Also returns the log probabilities of the action (the sampled caption) for REINFORCE.</span>

<span class="sd">        Args:</span>
<span class="sd">        ----</span>
<span class="sd">            inputs: torch.tensor(1, 1, embed_size)</span>
<span class="sd">                pre-processed image tensor.</span>
<span class="sd">            max_sequence_length: int</span>
<span class="sd">                Max length of sequence which the nodel should generate.</span>
<span class="sd">        Returns:</span>
<span class="sd">        ------</span>
<span class="sd">            output: list</span>
<span class="sd">                predicted sentence (list of tensor ids).</span>
<span class="sd">            log_probs: torch.Tensor</span>
<span class="sd">                log probabilities of the generated tokens (up to and including first END token)</span>
<span class="sd">            raw_outputs: torch.Tensor</span>
<span class="sd">                Raw logits for each prediction timestep.</span>
<span class="sd">            entropies: torch.Tesnor</span>
<span class="sd">                Entropies at each generation timestep.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># placeholders for output</span>
        <span class="n">output</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">raw_outputs</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># for structural loss computation</span>
        <span class="n">log_probs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">entropies</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">softmax</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">init_hiddens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">init_hidden</span><span class="p">(</span><span class="n">batch_size</span><span class="p">)</span>

        <span class="c1"># if torch.backends.mps.is_available():</span>
        <span class="c1">#     device = torch.device(&quot;mps&quot;)</span>
        <span class="c1"># elif torch.cuda.is_available():</span>
        <span class="c1">#     device = torch.device(&quot;cuda&quot;)</span>
        <span class="c1"># else:</span>
        <span class="c1">#     device = torch.device(&quot;cpu&quot;)</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span>

        <span class="c1">#### start sampling ####</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_sequence_length</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">cat_samples</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
                <span class="n">hidden_state</span> <span class="o">=</span> <span class="n">init_hiddens</span>

            <span class="n">cat_samples</span> <span class="o">=</span> <span class="n">cat_samples</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

            <span class="n">out</span><span class="p">,</span> <span class="n">hidden_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">cat_samples</span><span class="p">,</span> <span class="n">hidden_state</span><span class="p">)</span>
            
            <span class="c1"># get and save probabilities and save raw outputs</span>
            <span class="n">raw_outputs</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
            <span class="n">probs</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

            <span class="n">max_probs</span><span class="p">,</span> <span class="n">cat_samples</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">log_p</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">max_probs</span><span class="p">)</span>
            <span class="n">entropy</span> <span class="o">=</span> <span class="o">-</span><span class="n">log_p</span> <span class="o">*</span> <span class="n">max_probs</span>

            <span class="n">top5_probs</span><span class="p">,</span> <span class="n">top5_inds</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">topk</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

            <span class="n">entropies</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">entropy</span><span class="p">)</span>
            <span class="n">output</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cat_samples</span><span class="p">)</span>
            <span class="c1"># cat_samples = torch.cat((cat_samples, cat_samples), dim=-1)</span>
            <span class="c1"># print(&quot;Cat samples &quot;, cat_samples)</span>
            <span class="n">log_probs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">log_p</span><span class="p">)</span>


        <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># stack</span>
        <span class="n">log_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">log_probs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">entropies</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">entropies</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

        <span class="c1">####</span>
        <span class="c1"># get effective log prob and entropy values - the ones up to (including) END (word2idx = 1)</span>
        <span class="c1"># mask positions after END - both entropy and log P should be 0 at those positions</span>
        <span class="n">end_mask</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="c1"># include the END token</span>
        <span class="n">end_inds</span> <span class="o">=</span> <span class="n">end_mask</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">clamp_</span><span class="p">(</span><span class="nb">max</span><span class="o">=</span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="c1"># shape: (batch_size,)</span>
        <span class="k">for</span> <span class="n">pos</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">end_inds</span><span class="p">):</span>
            <span class="c1"># zero out log Ps and entropies</span>
            <span class="n">log_probs</span><span class="p">[</span><span class="n">pos</span><span class="p">,</span> <span class="n">i</span><span class="p">:]</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">entropies</span><span class="p">[</span><span class="n">pos</span><span class="p">,</span> <span class="n">i</span><span class="p">:]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="c1">####</span>

        <span class="n">raw_outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">raw_outputs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocabulary_size</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">log_probs</span><span class="p">,</span> <span class="n">raw_outputs</span><span class="p">,</span> <span class="n">entropies</span>
</pre></div>
</div>
</div>
</div>
<p>Instantiate the module (with appropriate specs), load weights and instantiate weights with pre-trained weights.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># decoder configs</span>
<span class="n">embed_size</span> <span class="o">=</span> <span class="mi">512</span>
<span class="n">visual_embed_size</span> <span class="o">=</span> <span class="mi">512</span>
<span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">512</span>

<span class="n">decoder</span> <span class="o">=</span> <span class="n">DecoderRNN</span><span class="p">(</span><span class="n">embed_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">visual_embed_size</span><span class="p">)</span>

<span class="c1"># Load the trained weights.</span>
<span class="n">decoder_file</span> <span class="o">=</span> <span class="s2">&quot;A3DS/pretrained_decoder_3dshapes.pkl&quot;</span>
<span class="n">decoder</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">decoder_file</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;All keys matched successfully&gt;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">itemID</span><span class="o">=</span><span class="mi">0</span>
<span class="n">image</span><span class="p">,</span> <span class="n">target_feats</span><span class="p">,</span> <span class="n">caption_text</span><span class="p">,</span> <span class="n">numeric_lbl</span><span class="p">,</span> <span class="n">caption_indx</span> <span class="o">=</span> <span class="n">A3DS_dataset</span><span class="o">.</span><span class="fm">__getitem__</span><span class="p">(</span><span class="n">itemID</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">caption_indx</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>tensor([ 0,  9, 17, 18,  4, 31, 38, 28, 15,  6, 16,  8,  9, 27, 11,  8, 12, 13,
         4, 45, 14,  1,  3,  3,  3,  3])
</pre></div>
</div>
</div>
</div>
<p>The current NIC module was actually trained for later use of two pictures (contrastive image captioning).
Therefore, we need to input the picture to be described not once, but twice.
(This is otherwise completely innocuous for our current purposes of single-picture captioning.)</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">target_features</span> <span class="o">=</span> <span class="n">target_feats</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">target_feats</span><span class="p">))</span>
<span class="n">both_images</span>     <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">target_features</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">target_features</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">output</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">decoder</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">both_images</span><span class="p">,</span> <span class="n">caption_indx</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

<span class="k">def</span> <span class="nf">clean_sentence</span><span class="p">(</span><span class="n">output</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Helper function for visualization purposes.</span>
<span class="sd">    Transforms list of token indices to a sentence.</span>
<span class="sd">    Also accepts mulit-dim tensors (for batch size &gt; 1).</span>

<span class="sd">    Args:</span>
<span class="sd">    ----</span>
<span class="sd">    output: torch.Tensor(batch_size, sentence_length)</span>
<span class="sd">        Tensor representing sentences in form of token indices.</span>

<span class="sd">    Returns:</span>
<span class="sd">    -------</span>
<span class="sd">    sentence: str</span>
<span class="sd">        String representing decoded sentences in natural language.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">list_string</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">output</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">idx</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">list_string</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">A3DS_dataset</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="s2">&quot;idx2word&quot;</span><span class="p">][</span><span class="n">i</span><span class="o">.</span><span class="n">item</span><span class="p">()])</span>
            <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">i</span><span class="p">:</span>
                    <span class="n">list_string</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">A3DS_dataset</span><span class="o">.</span><span class="n">vocab</span><span class="p">[</span><span class="s2">&quot;idx2word&quot;</span><span class="p">][</span><span class="n">y</span><span class="o">.</span><span class="n">item</span><span class="p">()])</span>
    <span class="n">sentence</span> <span class="o">=</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">list_string</span><span class="p">)</span> <span class="c1"># Convert list of strings to full string</span>
    <span class="n">sentence</span> <span class="o">=</span> <span class="n">sentence</span><span class="o">.</span><span class="n">capitalize</span><span class="p">()</span>  <span class="c1"># Capitalize the first letter of the first word</span>
    <span class="c1"># find index of end token for displaying</span>
    <span class="k">if</span> <span class="s2">&quot;end&quot;</span> <span class="ow">in</span> <span class="n">sentence</span><span class="p">:</span>
        <span class="n">len_sentence</span> <span class="o">=</span> <span class="n">sentence</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="p">)</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="s2">&quot;end&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">len_sentence</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">sentence</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="p">))</span>
    <span class="n">cleaned_sentence</span> <span class="o">=</span> <span class="s2">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">sentence</span><span class="o">.</span><span class="n">split</span><span class="p">()[:</span><span class="n">len_sentence</span><span class="p">])</span>
    <span class="k">return</span><span class="p">(</span><span class="n">cleaned_sentence</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">clean_sentence</span><span class="p">(</span><span class="n">output</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The picture shows a small orange cylinder in the left corner in front of a purple wall on red floor
</pre></div>
</div>
</div>
</div>
<p>Here is how we can compute the BLEU score for such generations:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torchtext.data.metrics</span> <span class="kn">import</span> <span class="n">bleu_score</span>

<span class="n">cleaned_sentence</span> <span class="o">=</span> <span class="n">clean_sentence</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
<span class="n">bleu1</span> <span class="o">=</span> <span class="n">bleu_score</span><span class="p">([</span><span class="n">cleaned_sentence</span><span class="o">.</span><span class="n">split</span><span class="p">()],</span> <span class="p">[</span> <span class="p">[</span><span class="n">s</span><span class="o">.</span><span class="n">split</span><span class="p">()</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">all_long_caps</span><span class="p">]],</span> <span class="n">max_n</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">weights</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;BLEU 1 &quot;</span><span class="p">,</span> <span class="n">bleu1</span><span class="p">)</span>

<span class="n">bleu2</span> <span class="o">=</span> <span class="n">bleu_score</span><span class="p">([</span><span class="n">cleaned_sentence</span><span class="o">.</span><span class="n">split</span><span class="p">()],</span> <span class="p">[</span> <span class="p">[</span><span class="n">s</span><span class="o">.</span><span class="n">split</span><span class="p">()</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">all_long_caps</span><span class="p">]],</span> <span class="n">max_n</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">weights</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;BLEU 2 &quot;</span><span class="p">,</span> <span class="n">bleu2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>BLEU 1  0.949999988079071
BLEU 2  0.9486833214759827
</pre></div>
</div>
</div>
</div>
<blockquote>
<div><p><strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 8.1.1: </span></strong></p>
<ol class="simple">
<li><p>[Just for yourself] Try out different images and generate captions for them. Try to get a feeling for how reliable or good they are. Try to figure out what criteria you use when you intuitively judge a caption as good. Think about what ’goodness’ of a generated caption means (also in relation to the ground truth in the training set).</p></li>
<li><p>Describe the architecture of the decoder module that is used in in direct comparison to the set up from the paper <a class="reference external" href="https://arxiv.org/abs/1411.4555">Vinyals et al. (2015)</a>. Highlight at least two differences in model architecture between the decoder model used here and that of Vinyals et al. These differences should all be <em>major</em> differences, i.e., differences that <em>could</em> plausible have a strong impact on the quality of the results. I.o.w., do not mention trivialities.</p></li>
<li><p>Name at least two things that would be important to know for a direct, close reproduction of Vinyals et al. results that are not or only insufficiently described in the paper.</p></li>
</ol>
</div></blockquote>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./08-grounded-LMs"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="08a-grounded-LMs.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Grounded Language Models</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="08c-NIC-pretrained.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Sheet 8.2: Using 🤗’s pretrained models for image captioning</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Michael Franke<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>