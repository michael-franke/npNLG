{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sheet 6.3: Decoding strategies\n",
    "\n",
    "Given a (blackbox) function that gives us a next-word probability, how do we use this to generate naturally sounding text?\n",
    "\n",
    "This tutorial explores a bunch of options, using the GTP-2 distribution provided by ðŸ¤—'s `transformer` package. \n",
    "\n",
    "The tutorial closely follows this blog post: https://huggingface.co/blog/how-to-generate\n",
    "\n",
    "We will look at the following decoding strategies (in this order):\n",
    "\n",
    "1. pure sampling\n",
    "2. soft-max sampling\n",
    "3. greedy sampling\n",
    "4. beam search\n",
    "5. top-$k$ sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation (imports, defs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import relevant packages\n",
    "import torch \n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# add the EOS token as PAD token to avoid warnings\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "# convenience function for nicer output\n",
    "def pretty_print(s):\n",
    "    print(\"Output:\\n\" + 100 * '-')\n",
    "    print(tokenizer.decode(s, skip_special_tokens=True))\n",
    "\n",
    "# encode context the generation is conditioned on\n",
    "input_ids = tokenizer.encode('I enjoy walking with my cute dog', return_tensors='pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pure sampling approach\n",
    "\n",
    "In a pure sampling approach, we just sample each next word with exactly the probability assigned to it by the LM. Notice that this process, therefore, is non-determinisitic. We can force replicable results, though, by setting a seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog Kass; we have a good book out, and a cab so she can hang out as she waits for us: A 404 - Love Noni \"Perhaps the best way to describe murder in the arts is to say\n"
     ]
    }
   ],
   "source": [
    "# set a seed for reproducibility (if you want)\n",
    "# torch.manual_seed(1996)\n",
    "\n",
    "# use function 'model.generate' from `transformer` package to sample by\n",
    "#  setting `do_sample=True` and knocking out `top_k` sampling (see below)\n",
    "sample_output = model.generate(\n",
    "    input_ids,        # context to continue\n",
    "    do_sample=True,   # use sampling (not beam search (see below))\n",
    "    max_length=50,    # return maximally 50 words (including the input given)\n",
    "    top_k=0           # just sample one word\n",
    ")\n",
    "\n",
    "pretty_print(sample_output[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 6.3.1: </span></strong>\n",
    ">\n",
    "> 1. How good is this production? Is it grammatical? Locally meaningful, globally meaningful?\n",
    ">\n",
    "> 2. [optional] Try sampling 100 single next-words for your initial sentence fragment. (Remember not to set a seed.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft-max sampling\n",
    "\n",
    "In soft-max sampling, the probablity of sampling word $w_i$ is $P_{\\text{sample}}(w_i \\mid w_{1:i-1}) \\propto \\exp (\\frac{1}{\\tau} P_{\\text{M}}(w_i \\mid w_{1:i-1}) )$, where $\\tau$ is a temperature parameter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog, but in the end she is a bit stubborn and stubborn and she is not very happy with me anymore. This is the problem with the dog. I had her being more aggressive and more aggressive with me. I\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# same as before but with `temperature`` parameter\n",
    "SM_sample_output = model.generate(\n",
    "    input_ids,        # context to continue\n",
    "    do_sample=True,   # use sampling (not beam search (see below))\n",
    "    max_length=50,    # return maximally 50 words (including the input given)\n",
    "    top_k=0,          # just sample one word\n",
    "    temperature=0.7   # soft-max temperature parameter\n",
    ")\n",
    "\n",
    "pretty_print(SM_sample_output[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 6.3.2: </span></strong>\n",
    ">\n",
    "> 1. How good is this production? Is it grammatical? Locally meaningful, globally meaningful?\n",
    ">\n",
    "> 2. Predict what will happen if you set $\\tau=5$. Try it out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy sampling\n",
    "\n",
    "In greedy sampling, we don't actually sample but just take the most likely next-word at every step.\n",
    "Greedy sampling is equivalent to setting $\\tau=0$ for soft-max sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with my dog. I'm not sure if I'll ever be able to walk with my dog.\n",
      "\n",
      "I'm not sure if I'll\n"
     ]
    }
   ],
   "source": [
    "# greedy sampling is the default of the `model.generate` function\n",
    "\n",
    "greedy_output = model.generate(input_ids, max_length=50)\n",
    "\n",
    "pretty_print(greedy_output[0])  \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 6.3.3: </span></strong>\n",
    ">\n",
    "> 1. How good is this production? Is it grammatical? Locally meaningful, globally meaningful?\n",
    ">\n",
    "> 2. Is greedy sampling guaranteed to select the most likely sequence? Or can it be led astray?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam search\n",
    "\n",
    "In simplified terms, beam search is a parallel search procedure that keeps a number $k$ of path probabilities open at each choice point, dropping the least likely as we go along. (There is actually no unanimity in what exactly beam search means for NLG.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog, but I'm not sure if I'll ever be able to walk with him again.\n",
      "\n",
      "I'm not sure if I'll ever be able to walk with him again. I'm not sure if I'll\n"
     ]
    }
   ],
   "source": [
    "# option `early_stopping` implies stopping when all beams reach the end-of-sentence token\n",
    "beam_output = model.generate(\n",
    "    input_ids, \n",
    "    max_length=50, \n",
    "    num_beams=5, \n",
    "    early_stopping=True\n",
    ") \n",
    "\n",
    "pretty_print(beam_output[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 6.3.4: </span></strong>\n",
    ">\n",
    "> 1. How good is this production? Is it grammatical? Locally meaningful, globally meaningful?\n",
    ">\n",
    "> 2. Try out the option `no_repeat_ngram_size=2` and see if it improves the results. This option supresses generation of $n$-grams of the given size. Play around with the number $n$ supplied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top-$k$ sampling\n",
    "\n",
    "This sampling scheme looks at the $k$ most likely next-words and samples from so that:\n",
    "\n",
    "$$P_{\\text{sample}}(w_i \\mid w_{1:i-1}) \\propto \n",
    "\\begin{cases} \n",
    "  P_{M}(w_i \\mid w_{1:i-1}) & \\text{if } w_i \\text{ in top-}k  \\\\\n",
    "  0 & \\text{otherwise}\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog, but it's not for me to try to please a puppy.\"\n",
      "\n",
      "Michele, who lives in Fort Meade, Ga., said she never really wanted to \"lose the love\" of her\n"
     ]
    }
   ],
   "source": [
    "top_k_output = model.generate(\n",
    "    input_ids, \n",
    "    do_sample=True, \n",
    "    max_length=50, \n",
    "    top_k=50          # setting `top_k` option triggers top-k sampling\n",
    ")\n",
    "\n",
    "pretty_print(top_k_output[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Task*: \n",
    "\n",
    "How good is this production? Is it grammatical? Locally meaningful, globally meaningful?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 6.3.5: </span></strong>\n",
    ">\n",
    "> 1. How good is this production? Is it grammatical? Locally meaningful, globally meaningful?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top-$p$ sampling\n",
    "\n",
    "Top-$p$ sampling is similar to top-$k$ sampling, but restricts sampling not to the top-$k$ most likely words (so always the same number of words), but the set of most likely words the summed probability of which exceeds threshold $p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "I enjoy walking with my cute dog and he's a really nice person. The second time we got to our house, my mother came over and brought a bucket of water. I'm very happy with it. She was just a little upset that I\n"
     ]
    }
   ],
   "source": [
    "top_k_output = model.generate(\n",
    "    input_ids, \n",
    "    do_sample=True, \n",
    "    max_length=50, \n",
    "    top_p=0.9        # set the top-p parameter here\n",
    ")\n",
    "\n",
    "pretty_print(top_k_output[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 6.3.6: </span></strong>\n",
    ">\n",
    "> 1. How good (grammatical, locally and globally coherent) is this output?\n",
    ">\n",
    "> 2. In which cases would the next-word predictions of top-$k$ and top-$p$ divergence quite a bit?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 6.3.7: Comparison of different decoding schemes. </span></strong>\n",
    ">\n",
    "> 1. Which of the decoding schemes included in this work sheet is a special case of which other decoding scheme(s)? E.g., X is a special case of Y if the behavior of Y is obtained when we set certain paramters of X to specific values."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1c7434731747d88815ea98a4470429bf1ebbf9287f8433fda1e0ecccc300fe0c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
