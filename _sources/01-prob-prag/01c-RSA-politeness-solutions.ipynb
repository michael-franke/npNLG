{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": "Solutions Sheet 1.2: RSA with politeness\n========================================\n\n**Author:** Michael Franke\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
      },
      "outputs": [],
      "source": [
        "##################################################\n## imports\n##################################################\n\nimport numpy as np\nimport seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n\n##################################################\n## helper functions\n##################################################\n\ndef softmax(x, axis=1):\n    \"\"\"\n    Softmax function in numpy\n    Parameters\n    ----------\n    x: array\n        An array with any dimensionality\n    axis: int\n        The axis along which to apply the softmax\n    Returns\n    -------\n    array\n        Same shape as x\n    \"\"\"\n    e_x = np.exp(x - np.max(x, axis, keepdims=True))\n    return e_x / e_x.sum(axis=axis, keepdims=True)\n\n\ndef normalize(arr, axis=1):\n    \"\"\"\n    Normalize arr along axis\n    \"\"\"\n    return arr / arr.sum(axis, keepdims=True)\n\n##################################################\n## defining the context\n##################################################\n\nstates     = [1,2,3,4,5]\nutterances = [\"terrible\",\"bad\",\"okay\",\"good\",\"amazing\"]\n\nsemantic_meaning = np.array(\n    [[.95 ,.85 ,.02 ,.02,.02],    # terrible\n     [.85 ,.95 ,.02 ,.02,.02],    # bad\n     [.02 ,.25 ,.95 ,.65,.35],    # okay\n     [.02 ,.05 ,.55 ,.95,.93],    # good\n     [.02 ,.02 ,.02 ,.65,.95]]    # amazing\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "> <strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 1.2.1: Check experiment and your intuitions</span></strong>\n>\n> 1. Consult the original paper ([Yoon et al. 2016](http://langcog.stanford.edu/papers_new/yoon-2016-cogsci.pdf)) to find the description of the experiment that was used to get these semantic values. Describe this experiment in at most three simple sentences: what was the question participants had to answer and how were answers recorded?\n> 2. Comment on whether you find the obtained values intuitive as values of the semantic meaning of these expression.\n> 3. Do you think that the experiment was well-designed for the task of eliciting information about semantic meaning of expressions?\n\n> <strong><span style=&ldquo;color:#D83D2B;&rdquo;>Solutions for Exercise 1.2.1: Check experiment and your intuitions</span></strong>\n>\n> 1. The experiment elicited binary judgements (&rsquo;yes&rsquo; and &rsquo;no&rsquo;) from participants. Each task presented two things: (i) a true state (Bob&rsquo;s true feeling about some item, shown as 1 to 5 out of 5 hearts), and (ii) a question &ldquo;Do you think that Bob thought Ann&rsquo;s cake was X?&rdquo; where X was one of the target words. The proportion of &rsquo;yes&rsquo; answers for each pair of (i) number of hearts and (ii) target word were used as the literal semantic value of that pair.\n> 2. The resulting values seem intuitive enough, at least when we look at the number of stars that received the highest rating for each word.\n> 3. The procedure is okay, as it is difficult to elicit semantic (truth) values. But it seems that this procedure (like many others) may slightly conflate the genuine underlying semantic meaning with a pragmatically enriched interpretation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "terrible     bad    okay    good  amazing  object\n1    0.6804  0.3196  0.0000  0.0000   0.0000       1\n2    0.2641  0.7355  0.0004  0.0000   0.0000       2\n3    0.0000  0.0000  0.9780  0.0220   0.0000       3\n4    0.0000  0.0000  0.0059  0.2180   0.7761       4\n5    0.0000  0.0000  0.0000  0.0113   0.9887       5\n<seaborn.axisgrid.FacetGrid at 0x1310d3e80>"
        },
        {
          "data": {
            "image/png": "",
            "text/plain": "<matplotlib.figure.Figure>"
          },
          "metadata": {
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "##################################################\n## model parameters\n##################################################\n\nalpha        = 10\nphi          = 0.99\nsocial_value = 1.25\n\n##################################################\n## RSA speaker with politeness\n##################################################\n\ndef RSA_polite_speaker(alpha, phi, social_value):\n    \"\"\"\n    predictions of an RSA model with politeness (speaker part)\n    (following: http://www.problang.org/chapters/09-politeness.html)\n    Parameters\n    ----------\n    alpha: float\n        Optimality parameter\n    phi: float\n        Relative weight of epistemic utility component\n    social_value: float\n        Social value factor (how much more \"socially valuable\" is one more star?)\n    Returns\n    -------\n    array\n        probability that speaker chooses utterance for each state\n    \"\"\"\n    literal_listener   = normalize(semantic_meaning)\n    epistemic_utility  = np.log(np.transpose(literal_listener))\n    social_utility     = np.sum(literal_listener * np.array([states]) * social_value, axis=1)\n    util_speaker       = phi * epistemic_utility + (1-phi) * social_utility\n    pragmatic_speaker  = softmax(alpha * util_speaker)\n    return(pragmatic_speaker)\n\nRSA_speaker_predictions = RSA_polite_speaker(alpha, phi, social_value)\n\n##################################################\n## showing and plotting the results\n##################################################\n\nspeaker  = pd.DataFrame(data    = RSA_speaker_predictions,\n                        index   = states,\n                        columns = utterances)\nspeaker['object'] = speaker.index\n\nprint(speaker.round(4))\n\nspeaker_long = speaker.melt(id_vars      = \"object\",\n                            var_name     = \"utterance\",\n                            value_name   = \"probability\",\n                            ignore_index = False)\nspeaker_plot = sns.FacetGrid(speaker_long, col=\"object\")\nspeaker_plot.map(sns.barplot, \"utterance\", \"probability\")\n# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "> <strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 1.2.2: Explore the polite speaker model</span></strong>\n>\n> 0. For yourself, in order to understand the model, go through each line of the definition of the function \\`RSA<sub>polite</sub><sub>speaker</sub>\\` and make sure that you understand what is happening.\n> 1. Change the call to the speaker to make it so that it only cares about making the listener feel good. What parameter value(s) did you choose?\n> 2. Change the call to the speaker to make it so that it cares about both making the listener feel good *and* conveying information. What parameter value(s) did you choose?\n> 3. If we set $\\varphi=1$, and choose a very high $\\alpha$, the speaker behavior is quite regular. Is this kind of behavior intuitive? Do you think it happens in real life?\n> 4. Is there a parameter setting for this model, such that the speaker would at most choose one category higher when trying to be polite? For example, when they should informatively say &rsquo;okay&rsquo;, they would say &rsquo;good&rsquo;, but never &rsquo;amazing&rsquo;. Which parameters achieve this? Or, how could the model be changed to achieve this behavior?\n\n> <strong><span style=&ldquo;color:#D83D2B;&rdquo;>Solutions Exercise 1.2.2: Explore the polite speaker model</span></strong>\n>\n> 1. To make the speaker only consider social utility, we set &ldquo;phi = 0&rdquo;.\n> 2. Any value of parameter &ldquo;phi&rdquo; that is not 0 or 1 takes both types of utility into account.\n> 3. Setting $\\varphi=1$, and choose a very high $\\alpha = 10,000$, the speaker only cares about informativity and is extremely &ldquo;rational&rdquo; (in the technical sense of utility maximization). It&rsquo;s dubious that such agents are implementable in neural wetware, and the result is rather extreme: there is exactly one word the speaker would send with near certainty in each state. Interestingly, the speaker is predicted to use &ldquo;amazing&rdquo; for both 4 and 5 stars, and not to use &ldquo;good&rdquo; at all. That is clearly unintuitive from a global perspective, because it is not optimal for achieving perfect communication. But it is what follows from the assume semantics. Since &ldquo;good&rdquo; got a rating of .55 for the 3 star-state, after normalization (literal listener), the probability to single out the 4 star state is higher with &ldquo;amazing&rdquo; than it is with &ldquo;good&rdquo;.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "terrible  bad  okay  good  amazing  object\n1       1.0  0.0   0.0   0.0      0.0       1\n2       0.0  1.0   0.0   0.0      0.0       2\n3       0.0  0.0   1.0   0.0      0.0       3\n4       0.0  0.0   0.0   0.0      1.0       4\n5       0.0  0.0   0.0   0.0      1.0       5"
        },
        {
          "data": {
            "image/png": "",
            "text/plain": "<matplotlib.figure.Figure>"
          },
          "metadata": {
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "alpha = 10000\nphi   = 1\nRSA_speaker_predictions = RSA_polite_speaker(alpha, phi, social_value)\n\n##################################################\n## showing and plotting the results\n##################################################\n\nspeaker  = pd.DataFrame(data    = RSA_speaker_predictions,\n                        index   = states,\n                        columns = utterances)\nspeaker['object'] = speaker.index\n\nprint(speaker.round(2))\n\nspeaker_long = speaker.melt(id_vars      = \"object\",\n                            var_name     = \"utterance\",\n                            value_name   = \"probability\",\n                            ignore_index = False)\nspeaker_plot = sns.FacetGrid(speaker_long, col=\"object\")\nspeaker_plot.map(sns.barplot, \"utterance\", \"probability\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "> 4. This is difficult with this model, if possible at all. We need a rather low value of $\\varphi$ to get the speaker to use &ldquo;good&rdquo; for sate 2, but then as $\\varphi$ gets lower and lower, the speaker starts to use &ldquo;amazing&rdquo; for state 1. The problem is that social utility is about making the speaker feel as good as possible, not just as &ldquo;a bit better than the true state&rdquo;. We could define an alternative social utility function which achieves this better than the current model. It would be an interesting empirical question which kind of social utility function best explains human judgements about polite language use. (Your project?)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "listener posterior over states after hearing 'good':\n [0.00588069 0.01970965 0.40323864 0.53989896 0.03127205]\nplotting listener posterior for utterance: good"
        },
        {
          "data": {
            "image/png": "",
            "text/plain": "<matplotlib.figure.Figure>"
          },
          "metadata": {
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "##################################################\n## pragmatic listener infers politeness level\n##################################################\n\n# which phi-values to consider\nphi_marks     = np.linspace(start=0, stop=1, num=11)\nphi_prior_flt = np.array([1,1,1,1,1,1,1,1,1,1,1])   # flat\nphi_prior_bsd = np.array([1,2,3,4,5,6,7,8,9,10,11]) # biased towards politeness\n\ndef RSA_polite_listener(alpha, phi_prior, social_value):\n    \"\"\"\n    predictions of an RSA model with politeness (listener part)\n    (following: http://www.problang.org/chapters/09-politeness.html)\n    Parameters\n    ----------\n    alpha: float\n        Optimality parameter\n    phi_priors: float\n        Prior over degree of politeness (phi-parameter)\n    social_value: float\n        Social value factor (how much more \"socially valuable\" is one more star?)\n    Returns\n    -------\n    array\n         for each message: listener posterior over state-phi pairs\n    \"\"\"\n    phi_prior = phi_prior / np.sum(phi_prior) # make sure priors are normalized\n    posterior = np.zeros((len(utterances), len(states),len(phi_marks)))\n    for i in range(len(phi_marks)):\n        pragmatic_speaker   = RSA_polite_speaker(alpha, phi_marks[i], social_value)\n        posterior_given_phi = normalize(np.transpose(pragmatic_speaker), axis=1)\n        posterior[:,:,i]    = posterior_given_phi * phi_prior[i]\n    return(posterior)\n\nRSA_listener_predictions = RSA_polite_listener(alpha, phi_prior_bsd, social_value)\n\nprint(\"listener posterior over states after hearing 'good':\\n\",\n      np.sum(RSA_listener_predictions[3,:,:], axis=1))\n\niterables=[utterances, states, phi_marks]\nindex = pd.MultiIndex.from_product(iterables, names=['utterances','states','phi'])\n\nlistener = pd.DataFrame(RSA_listener_predictions.reshape(RSA_listener_predictions.size, 1),\n                        index=index)\nlistener = listener.reset_index()\n\n##################################################\n## plotting the results\n##################################################\n\ndef plot_listener(utterance_index):\n    print(\"plotting listener posterior for utterance:\", utterances[utterance_index])\n    predictions = RSA_listener_predictions[utterance_index,:,:]\n    sns.heatmap(predictions)\n    plt.show()\n\nplot_listener(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "> <strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 1.2.3: Explore the pragmatic listener</span></strong>\n>\n> 0. For yourself, in order to understand the model, go through each line of the definition of the function \\`RSA<sub>polite</sub><sub>listener</sub>\\` and make sure that you understand what is happening.\n> 1. What does the heatmap show? What&rsquo;s on the x-axis, what&rsquo;s on the y-axis, and what do the colors mean?\n> 2. Add a function that takes an utterance index (0, &#x2026;, 4) and outputs three things: (i) a print out of the [marginal distribution](https://en.wikipedia.org/wiki/Marginal_distribution) over states, (ii) a print out of the marginal distribution over $\\varphi$ values, and (iii) the heatmap visualizing the joint-distribution of both.\n> 3. Compare the interpretation of the utterance &rsquo;amazing&rsquo; with that of the other utterances (for the parameter values used originally). Explain in what sense the distribution shown for &rsquo;amazing&rsquo; is a [multimodal distribution](https://en.wikipedia.org/wiki/Multimodal_distribution). Explain why the model makes this multi-modal prediction for &rsquo;amazing&rsquo;. Does it also predict multi-modality for &rsquo;good&rsquo;? What about &rsquo;terrible&rsquo;?\n\n> <strong><span style=&ldquo;color:#D83D2B;&rdquo;> Solutions Exercise 1.2.3: Explore the pragmatic listener</span></strong>\n>\n> 1. The heatmap has the five states (star ratings) on the $y$-axis, and the 11 values for $\\varphi$ that we consider here on the $x$ axis. The color coding shows the joint posterior probability for each pair of state and $\\varphi$ value.\n> 2. Here is a function that: (i) a print out of the [marginal distribution](https://en.wikipedia.org/wiki/Marginal_distribution) over states, (ii) a print out of the marginal distribution over $\\varphi$ values, and (iii) the heatmap visualizing the joint-distribution of both. This function directly computes the predictions, based on parameter input. This is for convenience (not strictly required from the exercise).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "listener posterior over states after hearing 'bad':\n [0.43  0.56  0.003 0.003 0.003]\nlistener posterior over phi-values after hearing 'bad':\n [0.015 0.03  0.045 0.061 0.076 0.091 0.106 0.121 0.136 0.152 0.167]"
        },
        {
          "data": {
            "image/png": "",
            "text/plain": "<matplotlib.figure.Figure>"
          },
          "metadata": {
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "def myFun(utterance_index, alpha, phi_prior, social_value):\n    RSA_listener_predictions = RSA_polite_listener(alpha, phi_prior, social_value)\n\n    iterables=[utterances, states, phi_marks]\n    index = pd.MultiIndex.from_product(iterables, names=['utterances','states','phi'])\n\n    listener = pd.DataFrame(RSA_listener_predictions.reshape(RSA_listener_predictions.size, 1),\n                            index=index)\n    listener = listener.reset_index()\n\n    # marginal over states\n    print(\"listener posterior over states after hearing '%s':\\n\" % utterances[utterance_index],\n          np.sum(RSA_listener_predictions[utterance_index,:,:], axis=1).round(3))\n    # marginal over phi values\n    print(\"listener posterior over phi-values after hearing '%s':\\n\" % utterances[utterance_index],\n          np.sum(RSA_listener_predictions[utterance_index,:,:], axis=0).round(3))\n    # plot of joint-posterior\n    predictions = RSA_listener_predictions[utterance_index,:,:]\n    sns.heatmap(predictions)\n    plt.show()\n\nmyFun(1, alpha, phi_prior_bsd, social_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "> 3. Here is posterior for &ldquo;amazing&rdquo;:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "listener posterior over states after hearing 'amazing':\n [0.072 0.038 0.01  0.386 0.494]\nlistener posterior over phi-values after hearing 'amazing':\n [0.015 0.03  0.045 0.061 0.076 0.091 0.106 0.121 0.136 0.152 0.167]"
        },
        {
          "data": {
            "image/png": "",
            "text/plain": "<matplotlib.figure.Figure>"
          },
          "metadata": {
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "myFun(4,alpha, phi_prior_bsd, social_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "And here it is for &ldquo;good&rdquo;:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "listener posterior over states after hearing 'good':\n [0.006 0.02  0.403 0.54  0.031]\nlistener posterior over phi-values after hearing 'good':\n [0.015 0.03  0.045 0.061 0.076 0.091 0.106 0.121 0.136 0.152 0.167]"
        },
        {
          "data": {
            "image/png": "",
            "text/plain": "<matplotlib.figure.Figure>"
          },
          "metadata": {
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "myFun(3,alpha, phi_prior_bsd, social_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "And &ldquo;terrible&rdquo;\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "listener posterior over states after hearing 'terrible':\n [0.746 0.245 0.003 0.003 0.003]\nlistener posterior over phi-values after hearing 'terrible':\n [0.015 0.03  0.045 0.061 0.076 0.091 0.106 0.121 0.136 0.152 0.167]"
        },
        {
          "data": {
            "image/png": "",
            "text/plain": "<matplotlib.figure.Figure>"
          },
          "metadata": {
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "myFun(0,alpha, phi_prior_bsd, social_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "While there are also two local peaks in the posterior of &ldquo;good&rdquo;, the case of &ldquo;amazing&rdquo; is most clearly multimodal.\nOne interpretation possibility is that the speaker really found the cookies terrible (1 star), but wanted didn&rsquo;t want to throw informativity overboard entirely ($\\varphi$ index 4, state index 0).\nAnother peak possibility is that the cookies really were amazing and that the speaker is maximally informative.\n(NB: the latter is a peak because of the biased prior.)\n\n"
      ]
    }
  ],
  "metadata": {
    "org": null,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
