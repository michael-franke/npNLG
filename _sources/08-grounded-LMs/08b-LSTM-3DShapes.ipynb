{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": "Sheet 8.1: An LSTM-based image captioner for the annotated 3D-Shapes data set\n=============================================================================\n\n**Author:** Michael Franke & Polina Tsvilodub\n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "In order to see how a custom-built neural image captioning (NIC) system can be implemented and used for inference, we look here at a relatively simple LSTM-based image captioner.\nTo shortcut the time it takes for training this model, we use downloadable weights from a trained version of this model.\nThe goal is to see the architecture in full detail and to get a feeling for the way in which images and text data is handled during inference.\nMost importantly, however, we will want to get a feeling for how good the model&rsquo;s predictions are, at least intuitively.\n\nThis tutorial uses a synthetic data set of annotations for the [3D Shapes data set](https://github.com/deepmind/3d-shapes), which we will refer to as &ldquo;annotated 3D Shapes data set&rdquo; or &ldquo;A3DS&rdquo; for short.\nOther image-captioning data sets (like MS-Coco) contain heterogeneous pictures and often only a small number of captions per picture, making it less clear whether it is the NIC&rsquo;s fault or the potentially poor quality of the data set that causes generations to be intuitively inadequate (garbled, untrue, over- or underspecified &#x2026;).\nIn contrast, using the A3DS data set, makes it easier to judge, on intuitive grounds, whether generated captions are any good.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "## Credits and origin of material\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "The material presented in this tutorial is in large part based on work conducted by Polina Tsvilodub for her 2022 MSc thesis &ldquo;Language Drift of Multi-Agent Communication Systems in Reference Games&rdquo;.\nIn particular, the annotated 3D Shapes data set and the LSTM-based architecture stems from this thesis.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "## Necessary files\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "You need additional files to run the code in this notebook.\nIf you are on CoLab use these commands to install.\n(Check if the files are installed in the right directory (&rsquo;A3DS&rsquo;) after unzipping).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
      },
      "outputs": [],
      "source": [
        "# !wget https://github.com/michael-franke/npNLG/raw/main/neural_pragmatic_nlg/data/A3DS/A3DS.zip\n# !unzip A3DS.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "## Packages\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "On top of the usual suspects, we will use the &rsquo;Image&rsquo; and &rsquo;torchvision&rsquo; package to process image data.\nWe need package &rsquo;pickle&rsquo; to load image data and pre-trained model weights.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
      },
      "outputs": [],
      "source": [
        "##################################################\n## import packages\n##################################################\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom torchtext.data import get_tokenizer\nfrom torchvision import transforms\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport os\nimport numpy as np\nimport json\nimport random\nfrom random import shuffle\n# from tqdm import tqdm\nimport pickle\n# import 5py\n\nimport warnings\nwarnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "## The A3DS data set\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "The following code gives us PyTorch &rsquo;Dataset&rsquo; and &rsquo;DataLoader&rsquo; objects, with which to handle a 1k-subset of images and annotations from the A3DS data set.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "### The &rsquo;Dataset&rsquo; object\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "Here is the definition of the &rsquo;Dataset&rsquo; object.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
      },
      "outputs": [],
      "source": [
        "class A3DS(Dataset):\n    \"\"\"\n    Dataset class for loading the dataset of images and captions from the 3dshapes dataset.\n\n    Arguments:\n    ---------\n    num_labels: int\n        Number of distinct captions to sample for each image. Relevant for using the dataloader for training models.\n    labels_type: str\n        \"long\" or \"short\". Indicates whether long or short captions should be used.\n    run_inference: bool\n        Flag indicating whether this dataset will be used for performing inference with a trained image captioner.\n    batch_size: int\n        Batch size. Has to be 1 in order to save the example image-caption pairs.\n    vocab_file: str\n        Name of vocab file.\n    start_token: str\n        Start token.\n    end_token: str\n        End token.\n    unk_token: str\n        Token to be used when encoding unknown tokens.\n    pad_token: str\n        Pad token to be used for padding captions tp max_sequence_length.\n    max_sequence_length: int\n        Length to which all captions are padded / truncated.\n    \"\"\"\n    def __init__(\n            self,\n            path=\"A3DS\",\n            num_labels=1, # number of ground truth labels to retrieve per image\n            labels_type=\"long\", # alternative: short\n            run_inference=False, # depending on this flag, check presence of model weights\n            batch_size=1,\n            vocab_file=\"vocab.pkl\",\n            start_token=\"START\",  # might be unnecessary since vocab file is fixed anyways\n            end_token=\"END\",\n            unk_token=\"UNK\",\n            pad_token=\"PAD\",\n            max_sequence_length=26, # important for padding length\n        ):\n\n        # check vocab file exists\n        assert os.path.exists(os.path.join(path, vocab_file)), \"Make sure the vocab file exists in the directory passed to the dataloader (see README)\"\n\n        # check if image file exists\n        assert (os.path.exists(os.path.join(path, \"sandbox_3Dshapes_1000.pkl\")) and os.path.join(path, \"sandbox_3Dshapes_resnet50_features_1000.pt\")), \"Make sure the sandbox dataset exists in the directory passed to the dataloader (see README)\"\n\n        if labels_type == \"long\":\n            assert num_labels <= 20, \"Maximally 20 distinct image-long caption pairs can be created for one image\"\n        else:\n            assert num_labels <= 27, \"Maximally 27 distinct image-short caption pairs can be created for one image\"\n\n        self.batch_size = batch_size\n        with open(os.path.join(path, vocab_file), \"rb\") as vf:\n            self.vocab = pickle.load(vf)\n\n        self.max_sequence_length = max_sequence_length\n        self.start_token = start_token\n        self.end_token = end_token\n        self.unk_token = unk_token\n        self.pad_token = pad_token\n        self.tokenizer = get_tokenizer(\"basic_english\")\n\n        self.embedded_imgs = torch.load(os.path.join(path, \"sandbox_3Dshapes_resnet50_features_1000.pt\"))\n        with open(os.path.join(path, \"sandbox_3Dshapes_1000.pkl\"), \"rb\") as f:\n            self.sandbox_file = pickle.load(f)\n            self.images = self.sandbox_file[\"images\"]\n            self.numeric_labels = self.sandbox_file[\"labels_numeric\"]\n            self.labels_long = self.sandbox_file[\"labels_long\"]\n            self.labels_short = self.sandbox_file[\"labels_short\"]\n\n        if labels_type == \"long\":\n            labels_ids_flat = [list(np.random.choice(range(len(self.labels_long[0])), num_labels, replace=False)) for i in range(len(self.images))]\n            self.labels_flat = [self.labels_long[i][l] for i, sublst in enumerate(labels_ids_flat) for l in sublst]\n            self.img_ids_flat = [id for id in range(len(self.images)) for i in range(num_labels)]\n        else:\n            labels_ids_flat = [list(np.random.choice(range(len(self.labels_short[0])), num_labels, replace=False)) for i in range(len(self.images))]\n            self.labels_flat = [self.labels_short[i][l] for i, sublst in enumerate(labels_ids_flat) for l in sublst]\n            self.img_ids_flat = [id for id in range(len(self.images)) for i in range(num_labels)]\n\n        # print(\"len labels ids flat \", len(labels_ids_flat))\n        # print(\"len labels flat \", len(self.labels_flat), self.labels_flat[:5])\n        # print(\"len image ids flat \", len(self.img_ids_flat), self.img_ids_flat[:5])\n\n    def __len__(self):\n        \"\"\"\n        Returns length of dataset.\n        \"\"\"\n        return len(self.img_ids_flat)\n\n    def __getitem__(self, idx):\n        \"\"\"\n        Iterator over the dataset.\n\n        Arguments:\n        ---------\n        idx: int\n            Index for accessing the flat image-caption pairs.\n\n        Returns:\n        -------\n        target_img: np.ndarray (64,64,3)\n            Original image.\n        target_features: torch.Tensor(2048,)\n            ResNet features of the image.\n        target_lbl: str\n            String caption.\n        numeric_lbl: np.ndarray (6,)\n            Original numeric image annotation.\n        target_caption: torch.Tensor(batch_size, 25)\n            Encoded caption.\n        \"\"\"\n        # access raw image corresponding to the index in the entire dataset\n        target_img = self.images[self.img_ids_flat[idx]]\n        # access caption\n        target_lbl = self.labels_flat[idx]\n        # access original numeric annotation of the image\n        numeric_lbl = self.numeric_labels[self.img_ids_flat[idx]]\n        # cast type\n        target_img = np.asarray(target_img).astype('uint8')\n        # retrieve ResNet features, accessed through original image ID\n        target_features = self.embedded_imgs[self.img_ids_flat[idx]]\n        # tokenize label\n        tokens = self.tokenizer(str(target_lbl).lower().replace(\"-\", \" \"))\n        # Convert caption to tensor of word ids, append start and end tokens.\n        target_caption = self.tokenize_caption(tokens)\n        # convert to tensor\n        target_caption = torch.Tensor(target_caption).long()\n\n        return target_img, target_features, target_lbl, numeric_lbl, target_caption\n\n    def tokenize_caption(self, label):\n        \"\"\"\n        Helper for converting list of tokens into list of token IDs.\n        Expects tokenized caption as input.\n\n        Arguments:\n        --------\n        label: list\n            Tokenized caption.\n\n        Returns:\n        -------\n        tokens: list\n            List of token IDs, prepended with start, end, padded to max length.\n        \"\"\"\n        label = label[:(self.max_sequence_length-2)]\n        tokens = [self.vocab[\"word2idx\"][self.start_token]]\n        for t in label:\n            try:\n                tokens.append(self.vocab[\"word2idx\"][t])\n            except:\n                tokens.append(self.vocab[\"word2idx\"][self.unk_token])\n        tokens.append(self.vocab[\"word2idx\"][self.end_token])\n        # pad\n        while len(tokens) < self.max_sequence_length:\n            tokens.append(self.vocab[\"word2idx\"][self.pad_token])\n\n        return tokens\n\n    def get_labels_for_image(self, id, caption_type=\"long\"):\n        \"\"\"\n        Helper for getting all annotations for a given image id.\n\n        Arguments:\n        ---------\n        id: int\n            Index of image caption pair containing the image\n            for which the full list of captions should be returned.\n        caption_type: str\n            \"long\" or \"short\". Indicates type of captions to provide.\n\n        Returns:\n        -------\n            List of all captions for given image.\n        \"\"\"\n        if caption_type == \"long\":\n            return self.labels_long[self.img_ids_flat[id]]\n        else:\n            return self.labels_short[self.img_ids_flat[id]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "Lets instantiate the &rsquo;Dataset&rsquo; object and explore the structure of the A3DS data.\nNotice that there are a 1000 items in this subset of the A3DS data set.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "1000"
        }
      ],
      "source": [
        "A3DS_dataset = A3DS()\nprint(A3DS_dataset.__len__())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "Let&rsquo;s get a single item by some ID, here taking the first item.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
      },
      "outputs": [],
      "source": [
        "itemID=0\nimage, target_features, caption_text, numeric_lbl, caption_indx = A3DS_dataset.__getitem__(itemID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "Each item is a tuple with 5 pieces of information.\nFor our purposes, the most important ones are in slot 0 (the image information) and in slot 2 (the caption as a text).\n\nLet&rsquo;s have a look at the image, which is stored as a tensor.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "#+begin_example\n[[[153 226 249]\n  [153 226 249]\n  [153 226 249]\n  ...\n  [153 226 249]\n  [153 226 249]\n  [153 226 249]]\n\n [[153 226 249]\n  [153 226 249]\n  [153 226 249]\n  ...\n  [153 226 249]\n  [153 226 249]\n  [153 226 249]]\n\n [[153 226 249]\n  [153 226 249]\n  [153 226 249]\n  ...\n  [153 226 249]\n  [153 226 249]\n  [153 226 249]]\n\n ...\n\n [[254   0   0]\n  [254   0   0]\n  [253   0   0]\n  ...\n  [214   0   0]\n  [216   0   0]\n  [219   0   0]]\n\n [[251   0   0]\n  [246   0   0]\n  [250   0   0]\n  ...\n  [220   0   0]\n  [215   0   0]\n  [212   0   0]]\n\n [[255   0   0]\n  [248   0   0]\n  [243   0   0]\n  ...\n  [219   0   0]\n  [219   0   0]\n  [217   0   0]]]\n#+end_example"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": "<matplotlib.figure.Figure>"
          },
          "metadata": {
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# picture\nprint(image)\n\n# plot image\nplt.imshow(image)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "And here is a caption that goes with this picture.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
      },
      "outputs": [],
      "source": [
        "# ground-truth caption\nprint(caption_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "There are actually long and short captions for each image.\nWe have created an instance of the data set with one random long caption per image.\nWe can inspect the full list of short captions like so:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "#+begin_example\nthere is a small cylinder\nthere is a orange cylinder\nthere is a cylinder in the left corner\nthere is a cylinder in front of a purple wall\nthere is a cylinder on red floor\nthere is a small cylinder in the left corner\nthere is a small cylinder in front of a purple wall\nthere is a small cylinder on red floor\nthere is a orange cylinder in the left corner\nthere is a orange cylinder in front of a purple wall\nthere is a orange cylinder on red floor\na small cylinder\na orange cylinder\na cylinder in the left corner\na cylinder in front of a purple wall\na cylinder on red floor\na small cylinder in the left corner\na small cylinder in front of a purple wall\na small cylinder on red floor\na orange cylinder in the left corner\na orange cylinder in front of a purple wall\na orange cylinder on red floor\nthe cylinder is in the left corner\nthe cylinder is in front of a purple wall\nthe cylinder is on red floor\nthe cylinder is small\nthe cylinder is orange\n#+end_example"
        }
      ],
      "source": [
        "# Retrieve all short-captions for the image ID:\nall_short_caps = A3DS_dataset.get_labels_for_image(itemID, caption_type='short')\nfor c in all_short_caps:\n    print(c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "And similarly for the long captions.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "#+begin_example\na small orange cylinder in the left corner in front of a purple wall on red floor\na small orange cylinder in the left corner on red floor in front of a purple wall\na small orange cylinder on red floor in the left corner in front of a purple wall\na small orange cylinder on red floor in front of a purple wall in the left corner\nthe picture shows a small orange cylinder in the left corner in front of a purple wall on red floor\nthe picture shows a small orange cylinder in the left corner on red floor in front of a purple wall\nthe picture shows a small orange cylinder on red floor in the left corner in front of a purple wall\nthe picture shows a small orange cylinder on red floor in front of a purple wall in the left corner\na small orange cylinder located in the left corner in front of a purple wall on red floor\na small orange cylinder located in the left corner on red floor in front of a purple\na small orange cylinder located on red floor in the left corner in front of a purple wall\na small orange cylinder located on red floor in front of a purple wall in the left corner\nthe small cylinder in the left corner in front of a purple wall on red floor is orange\nthe small cylinder in the left corner on red floor in front of a purple wall is orange\nthe small cylinder on red floor in the left corner in front of a purple wall is orange\nthe small cylinder on red floor in front of a purple wall in the left corner is orange\nthe orange cylinder in the left corner in front of a purple wall on red floor is small\nthe orange cylinder in the left corner on red floor in front of a purple wall is small\nthe orange cylinder on red floor in the left corner in front of a purple wall is small\nthe orange cylinder on red floor in front of a purple wall in the left corner is small\n#+end_example"
        }
      ],
      "source": [
        "# Retrieve all long-captions for the image ID:\n\nall_long_caps = A3DS_dataset.get_labels_for_image(itemID, caption_type='long')\nfor c in all_long_caps:\n    print(c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "Finally, let&rsquo;s also have a look at the vocabulary for this A3DS data set:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "VOCAB:  dict_keys(['START', 'END', 'UNK', 'PAD', 'a', 'tiny', 'red', 'block', 'in', 'the', 'right', 'corner', 'front', 'of', 'wall', 'on', 'floor', 'picture', 'shows', 'standing', 'is', 'close', 'to', 'side', 'near', 'middle', 'nearly', 'left', 'cylinder', 'ball', 'pill', 'small', 'medium', 'sized', 'big', 'large', 'huge', 'giant', 'orange', 'yellow', 'light', 'green', 'dark', 'cyan', 'blue', 'purple', 'pink'])\nVOCAB SIZE:  47"
        }
      ],
      "source": [
        "vocab = A3DS_dataset.vocab[\"word2idx\"].keys()\nprint(\"VOCAB: \", vocab)\n\nvocab_size = len(vocab)\nprint(\"VOCAB SIZE: \", vocab_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "We see that this vocabulary is actually pretty small.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "### Creating a &rsquo;DataLoader&rsquo;\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "Let&rsquo;s create a &rsquo;DataLoader&rsquo; for batches of a specified size, using a random shuffle of the data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
      },
      "outputs": [],
      "source": [
        "batch_size = 50\nA3DS_data_loader = torch.utils.data.DataLoader(\n    dataset    = A3DS_dataset,\n    batch_size = batch_size,\n    shuffle    = True,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "## The (pre-trained) LSTM NIC\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "Definition of the LSTM-based neural image captioner as an instance of PyTorch&rsquo;s &rsquo;nn.Module&rsquo;:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
      },
      "outputs": [],
      "source": [
        "class DecoderRNN(nn.Module):\n    def __init__(self, embed_size, hidden_size, vocab_size, visual_embed_size, batch_size=1, num_layers=1):\n        \"\"\"\n        Initialize the language module consisting of a one-layer LSTM and\n        trainable embeddings. The image embeddings (both target and distractor!)\n        are used as additional context at every step of the training\n        (prepended to each word embedding).\n\n        Args:\n        -----\n            embed_size: int\n                Dimensionality of trainable embeddings.\n            hidden_size: int\n                Hidden/ cell state dimensionality of the LSTM.\n            vocab_size: int\n                Length of vocabulary.\n            visual_embed_size: int\n                Dimensionality of each image embedding to be appended at each time step as additional context.\n            batch_size: int\n                Batch size.\n            num_layers: int\n                Number of LSTM layers.\n        \"\"\"\n        super(DecoderRNN, self).__init__()\n        self.num_layers = num_layers\n        self.hidden_size = hidden_size\n        self.embed_size= embed_size\n        self.vocabulary_size = vocab_size\n        self.visual_embed_size = visual_embed_size\n        # embedding layer\n        self.embed = nn.Embedding(self.vocabulary_size, self.embed_size)\n        # layer projecting ResNet features of a single image to desired size\n        self.project = nn.Linear(2048, self.visual_embed_size)\n\n      # LSTM takes as input the word embedding with prepended embeddings of the two images at each time step\n        # note that the batch dimension comes first\n        self.lstm = nn.LSTM(self.embed_size + 2*self.visual_embed_size, self.hidden_size , self.num_layers, batch_first=True)\n        # transforming last lstm hidden state to scores over vocabulary\n        self.linear = nn.Linear(hidden_size, self.vocabulary_size)\n\n        self.batch_size = batch_size\n        # initial hidden state of the lstm\n        self.hidden = self.init_hidden(self.batch_size)\n\n        # initialization of the layers\n        self.embed.weight.data.uniform_(-0.1, 0.1)\n        self.linear.weight.data.uniform_(-0.1, 0.1)\n        self.linear.bias.data.fill_(0)\n\n    def init_hidden(self, batch_size):\n\n        \"\"\"\n        At the start of training, we need to initialize a hidden state;\n        Defines a hidden state with all zeroes\n        The axes are (num_layers, batch_size, hidden_size)\n        \"\"\"\n        # if torch.backends.mps.is_available():\n        #     device = torch.device(\"mps\")\n        # elif torch.cuda.is_available():\n        #     device = torch.device(\"cuda\")\n        # else:\n        #     device = torch.device(\"cpu\")\n        device = torch.device('cpu')\n\n        return (torch.zeros((1, batch_size, self.hidden_size), device=device), \\\n                torch.zeros((1, batch_size, self.hidden_size), device=device))\n\n    def forward(self, features, captions, prev_hidden):\n        \"\"\"\n        Perform forward step through the LSTM.\n\n        Args:\n        -----\n            features: torch.tensor(batch_size, 2, embed_size)\n                Embeddings of images, target and distractor concatenated in this order.\n            captions: torch.tensor(batch_size, caption_length)\n                Lists of indices representing tokens of each caption.\n            prev_hidden: (torch.tensor(num_layers, batch_size, hidden_size), torch.tensor(num_layers, batch_size, hidden_size))\n                Tuple containing previous hidden and cell states of the LSTM.\n        Returns:\n        ------\n            outputs: torch.tensor(batch_size, caption_length, embedding_dim)\n                Scores over vocabulary for each token in each caption.\n            hidden_state: (torch.tensor(num_layers, batch_size, hidden_size), torch.tensor(num_layers, batch_size, hidden_size))\n                Tuple containing new hidden and cell state of the LSTM.\n        \"\"\"\n\n        # features of shape (batch_size, 2, 2048)\n        image_emb = self.project(features) # image_emb should have shape (batch_size, 2, 512)\n        # concatenate target and distractor embeddings\n        img_features = torch.cat((image_emb[:, 0, :], image_emb[:, 1, :]), dim=-1).unsqueeze(1)\n        embeddings = self.embed(captions)\n        # repeat image features such that they can be prepended to each token\n        img_features_reps = img_features.repeat(1, embeddings.shape[1], 1)\n        # PREpend the feature embedding as additional context as first token, assume there is no END token\n        embeddings = torch.cat((img_features_reps, embeddings), dim=-1)\n        out, hidden_state = self.lstm(embeddings, prev_hidden)\n        # project LSTM predictions on to vocab\n        outputs = self.linear(out) # prediction shape is (batch_size, max_sequence_length, vocab_size)\n        # print(\"outputs shape in forward \", outputs.shape)\n        return outputs, hidden_state\n\n    def log_prob_helper(self, logits, values):\n        \"\"\"\n        Helper function for scoring the sampled token,\n        because it is not implemented for MPS yet.\n        Just duplicates source code from PyTorch.\n        \"\"\"\n        values = values.long().unsqueeze(-1)\n        values, log_pmf = torch.broadcast_tensors(values, logits)\n        values = values[..., :1]\n        return log_pmf.gather(-1, values).squeeze(-1)\n\n    def sample(self, inputs, max_sequence_length):\n        \"\"\"\n        Function for sampling a caption during functional (reference game) training.\n        Implements greedy sampling. Sampling stops when END token is sampled or when max_sequence_length is reached.\n        Also returns the log probabilities of the action (the sampled caption) for REINFORCE.\n\n        Args:\n        ----\n            inputs: torch.tensor(1, 1, embed_size)\n                pre-processed image tensor.\n            max_sequence_length: int\n                Max length of sequence which the nodel should generate.\n        Returns:\n        ------\n            output: list\n                predicted sentence (list of tensor ids).\n            log_probs: torch.Tensor\n                log probabilities of the generated tokens (up to and including first END token)\n            raw_outputs: torch.Tensor\n                Raw logits for each prediction timestep.\n            entropies: torch.Tesnor\n                Entropies at each generation timestep.\n        \"\"\"\n\n        # placeholders for output\n        output = []\n        raw_outputs = [] # for structural loss computation\n        log_probs = []\n        entropies = []\n        batch_size = inputs.shape[0]\n        softmax = nn.Softmax(dim=-1)\n        init_hiddens = self.init_hidden(batch_size)\n\n        # if torch.backends.mps.is_available():\n        #     device = torch.device(\"mps\")\n        # elif torch.cuda.is_available():\n        #     device = torch.device(\"cuda\")\n        # else:\n        #     device = torch.device(\"cpu\")\n        device = torch.device('cpu')\n\n        #### start sampling ####\n        for i in range(max_sequence_length):\n            if i == 0:\n                cat_samples = torch.tensor([0]).repeat(batch_size, 1)\n                hidden_state = init_hiddens\n\n            cat_samples = cat_samples.to(device)\n            inputs = inputs.to(device)\n\n            out, hidden_state = self.forward(inputs, cat_samples, hidden_state)\n            \n            # get and save probabilities and save raw outputs\n            raw_outputs.extend(out)\n            probs = softmax(out)\n\n            max_probs, cat_samples = torch.max(probs, dim = -1)\n            log_p = torch.log(max_probs)\n            entropy = -log_p * max_probs\n\n            top5_probs, top5_inds = torch.topk(probs, 5, dim=-1)\n\n            entropies.append(entropy)\n            output.append(cat_samples)\n            # cat_samples = torch.cat((cat_samples, cat_samples), dim=-1)\n            # print(\"Cat samples \", cat_samples)\n            log_probs.append(log_p)\n\n\n        output = torch.stack(output, dim=-1).squeeze(1)\n        # stack\n        log_probs = torch.stack(log_probs, dim=1).squeeze(-1)\n        entropies = torch.stack(entropies, dim=1).squeeze(-1)\n\n        ####\n        # get effective log prob and entropy values - the ones up to (including) END (word2idx = 1)\n        # mask positions after END - both entropy and log P should be 0 at those positions\n        end_mask = output.size(-1) - (torch.eq(output, 1).to(torch.int64).cumsum(dim=1) > 0).sum(dim=-1)\n        # include the END token\n        end_inds = end_mask.add_(1).clamp_(max=output.size(-1)) # shape: (batch_size,)\n        for pos, i in enumerate(end_inds):\n            # zero out log Ps and entropies\n            log_probs[pos, i:] = 0\n            entropies[pos, i:] = 0\n        ####\n\n        raw_outputs = torch.stack(raw_outputs, dim=1).view(batch_size, -1, self.vocabulary_size)\n        return output, log_probs, raw_outputs, entropies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "Instantiate the module (with appropriate specs), load weights and instantiate weights with pre-trained weights.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "<All keys matched successfully>"
        }
      ],
      "source": [
        "# decoder configs\nembed_size = 512\nvisual_embed_size = 512\nhidden_size = 512\n\ndecoder = DecoderRNN(embed_size, hidden_size, vocab_size, visual_embed_size)\n\n# Load the trained weights.\ndecoder_file = \"A3DS/pretrained_decoder_3dshapes.pkl\"\ndecoder.load_state_dict(torch.load(decoder_file))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "tensor([ 0,  9, 38, 28,  8,  9, 27, 11, 15,  6, 16,  8, 12, 13,  4, 45, 14, 20,\n        31,  1,  3,  3,  3,  3,  3,  3])\nhi!"
        }
      ],
      "source": [
        "itemID=0\nimage, target_feats, caption_text, numeric_lbl, caption_indx = A3DS_dataset.__getitem__(itemID)\nprint(caption_indx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "The current NIC module was actually trained for later use of two pictures (contrastive image captioning).\nTherefore, we need to input the picture to be described not once, but twice.\n(This is otherwise completely innocuous for our current purposes of single-picture captioning.)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
      },
      "outputs": [],
      "source": [
        "target_features = target_feats.reshape(1,len(target_feats))\nboth_images     = torch.cat((target_features.unsqueeze(1), target_features.unsqueeze(1)), dim=1)\noutput, _, _, _ = decoder.sample(both_images, caption_indx.shape[0])\n\ndef clean_sentence(output):\n    \"\"\"\n    Helper function for visualization purposes.\n    Transforms list of token indices to a sentence.\n    Also accepts mulit-dim tensors (for batch size > 1).\n\n    Args:\n    ----\n    output: torch.Tensor(batch_size, sentence_length)\n        Tensor representing sentences in form of token indices.\n\n    Returns:\n    -------\n    sentence: str\n        String representing decoded sentences in natural language.\n    \"\"\"\n    list_string = []\n    for idx in output:\n        for i in idx:\n            try:\n                list_string.append(A3DS_dataset.vocab[\"idx2word\"][i.item()])\n            except ValueError:\n                for y in i:\n                    list_string.append(A3DS_dataset.vocab[\"idx2word\"][y.item()])\n    sentence = ' '.join(list_string) # Convert list of strings to full string\n    sentence = sentence.capitalize()  # Capitalize the first letter of the first word\n    # find index of end token for displaying\n    if \"end\" in sentence:\n        len_sentence = sentence.split(\" \").index(\"end\")\n    else:\n        len_sentence = len(sentence.split(\" \"))\n    cleaned_sentence = \" \".join(sentence.split()[:len_sentence])\n    return(cleaned_sentence)\n\nprint(clean_sentence(output))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "> <strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 8.1.1: </span></strong>\n>\n> 0. [Just for yourself] Try out different images and generate captions for them. Try to get a feeling for how reliable or good they are. Try to figure out what criteria you use when you intuitively judge a caption as good. Think about what &rsquo;goodness&rsquo; of a generated caption means (also in relation to the ground truth in the training set).\n>\n> 1. Describe the architecture of the decoder module that is used in in direct comparison to the set up from the paper [Vinyals et al. (2015)](https://arxiv.org/abs/1411.4555). Highlight at least two differences in model architecture between the decoder model used here and that of Vinyals et al. These differences should all be *major* differences, i.e., differences that *could* plausible have a strong impact on the quality of the results. I.o.w., do not mention trivialities.\n>\n> 2. Name at least two things that would be important to know for a direct, close reproduction of Vinyals et al. results that are not or only insufficiently described in the paper.\n\n"
      ]
    }
  ],
  "metadata": {
    "org": null,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
