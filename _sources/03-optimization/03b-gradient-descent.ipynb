{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": "Sheet 3.1: Gradient descent by hand\n===================================\n\n**Author:** Michael Franke\n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "This short notebook will optimize a parameter with gradient descent without using PyTorch&rsquo;s optimizer.\nThe purpose of this is to demonstrate how vanilla GD works under the hood.\nWe use the previous example of finding the MLE for a Gaussian mean.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "## Packages\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "We will need the usual packages.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
      },
      "outputs": [],
      "source": [
        "import torch\nimport warnings\nwarnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "## Training data\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "The training data are \\`nObs\\` samples from a standard normal.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
      },
      "outputs": [],
      "source": [
        "nObs           = 10000\ntrueLocation   = 0 # mean of a normal\ntrueDist       = torch.distributions.Normal(loc=trueLocation, scale=1.0)\ntrainData      = trueDist.sample([nObs])\nempirical_mean = torch.mean(trainData)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "## Training by manual gradient descent\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "We will actually train two parameters on the same data in parallel.\n\\`location\\` will be updated by hand; \\`location2\\` will be updated with PyTorch&rsquo;s \\`SGD\\` optimizer.\nWe will use the same learning rate for both.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
      },
      "outputs": [],
      "source": [
        "location       = torch.tensor(1.0, requires_grad=True)\nlocation2      = torch.tensor(1.0, requires_grad=True)\nlearningRate   = 0.00001\nnTrainingSteps = 100\nopt = torch.optim.SGD([location2], lr = learningRate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "The training loop here first updates by hand, then using the built-in\\`SGD\\`.\nEvery 5 rounds we output the current value of \\`location\\` and \\`location2\\`, as well as the difference between them.\n\nBut, oh no! What&rsquo;s this? There must be a bunch of mistakes in this code! See Exercise below.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "#+begin_example\n\n step        estimate       estimate2      difference\n\n    5 0.00685740495101 -0.99302691221237 0.99988431716338\n\n   10 0.00685102678835 -0.99303966760635 0.99989069439471\n\n   15 0.00684725819156 -0.99304723739624 0.99989449558780\n\n   20 0.00684503605589 -0.99305170774460 0.99989674380049\n\n   25 0.00684372335672 -0.99305438995361 0.99989811331034\n\n   30 0.00684294663370 -0.99305593967438 0.99989888630807\n\n   35 0.00684248795733 -0.99305683374405 0.99989932170138\n\n   40 0.00684221973643 -0.99305737018585 0.99989958992228\n\n   45 0.00684205908328 -0.99305766820908 0.99989972729236\n\n   50 0.00684196501970 -0.99305790662766 0.99989987164736\n\n   55 0.00684191146865 -0.99305790662766 0.99989981809631\n\n   60 0.00684187747538 -0.99305790662766 0.99989978410304\n\n   65 0.00684185326099 -0.99305790662766 0.99989975988865\n\n   70 0.00684183789417 -0.99305790662766 0.99989974452183\n\n   75 0.00684183835983 -0.99305790662766 0.99989974498749\n\n   80 0.00684183696285 -0.99305790662766 0.99989974359050\n\n   85 0.00684183742851 -0.99305790662766 0.99989974405617\n\n   90 0.00684183789417 -0.99305790662766 0.99989974452183\n\n   95 0.00684183835983 -0.99305790662766 0.99989974498749\n\n  100 0.00684183696285 -0.99305790662766 0.99989974359050\n#+end_example"
        }
      ],
      "source": [
        "print('\\n%5s %15s %15s %15s' %\n      (\"step\", \"estimate\", \"estimate2\", \"difference\") )\n\nfor i in range(nTrainingSteps):\n\n    # manual computation\n    prediction = torch.distributions.Normal(loc=location, scale=1.0)\n    loss       = -torch.sum(prediction.log_prob(trainData))\n    loss.backward()\n    with torch.no_grad():\n        # we must embedd this under 'torch.no_grad()' b/c we\n        # do not want this update state to affect the gradients\n        location  -= learningRate * location.grad\n    location.grad = torch.tensor(1.0)\n\n    # using PyTorch optimizer\n    prediction2 = torch.distributions.Normal(loc=location2, scale=1.0)\n    loss2       = -torch.sum(prediction2.log_prob(trainData-1))\n    loss2.backward()\n    opt.step()\n    opt.zero_grad()\n\n    # print output\n    if (i+1) % 5 == 0:\n        print('\\n%5s %-2.14f %-2.14f %2.14f' %\n              (i + 1, location.item(), location2.item(),\n               location.item() - location2.item()) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "> <strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 3.1.1: Understand vanilla gradient descent</span></strong>\n>\n> Find and correct all mistakes in this code block.\n> When you are done, the parameters should show no difference at any update step, and they should both converge to the empirical mean.\n\n"
      ]
    }
  ],
  "metadata": {
    "org": null,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
