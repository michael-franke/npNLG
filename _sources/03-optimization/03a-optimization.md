
# Optimization in PyTorch

Optimization methods for neural networks are for the most part variations on Gradient Descent.
In this session, we will look at vanilla GD, and some of its descendants.


## Learning goals for this session

1.  Understand the basics of gradient descent.
2.  Get familiar with variations of GD.
3.  Learn how to use GD to optimize an RSA model.


## Additional resources

There are may great resources covering gradient descent methods.
To single out one concise overview, there is [this paper](https://arxiv.org/abs/1609.04747) supported by [this blog post](https://ruder.io/optimizing-gradient-descent/).

