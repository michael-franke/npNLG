{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": "Sheet 3.2: Optimizing an RSA model\n==================================\n\n**Author:** Michael Franke\n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "Here we will explore how to use PyTorch to find optimized values for the parameters of a vanilla RSA model for reference games.\nThis serves several purposes: (i) it provides a chance to exercise with the basics of parameter optimization in PyTorch; and (ii) we learn to think about models as objects that can (and must!) be critically tested with respect to their predictive ability.\n\nTo fit a vanilla RSA model, we use data from [Qing & Franke (2016)](https://michael-franke.github.io/heimseite/Papers/QingFranke_2013_Variations_on_Bayes.pdf). A Bayesian data analysis for this data set and model set up is provided in [this chapter of problang.org](http://www.problang.org/chapters/app-04-BDA.html).\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "## Packages\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "We will need to import the \\`torch\\` package for the main functionality.\nIn order to have a convenient handle, we load the \\`torch.nn.functional\\` package into variable \\`F\\`.\nWe use this to refer to the normalization function for tensors: \\`F.normalize\\`.\nWe use the \\`warnings\\` package to suppress all warning messages in the notebook.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
      },
      "outputs": [],
      "source": [
        "import torch\nimport torch.nn.functional as F\nimport warnings\nwarnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "## Context model\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "The context model for the reference game is the same as we used before (in Sheet 1.1).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
      },
      "outputs": [],
      "source": [
        "##################################################\n## data to fit\n##################################################\n\nobject_names     = ['blue_circle', 'green_square', 'blue_square']\nutterance_names  = ['blue', 'circle', 'green', 'square']\nsemantic_meaning = torch.tensor(\n    # blue circle, green square, blue square\n    [[1, 0, 1],  # blue\n     [1, 0, 0],  # circle\n     [0, 1, 0],  # green\n     [0, 1, 1]],  # square,\n    dtype= torch.float32\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "## The empirical data\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "We use empirical data from [Qing & Franke (2016)](https://michael-franke.github.io/heimseite/Papers/QingFranke_2013_Variations_on_Bayes.pdf).\nThere were three tasks: (i) speaker production choice, and (ii) listener interpretation choice, and (iii) salience prior elicitation.\nAll three tasks were *forced-choice tasks*, in which participants had to select a single option from a small list of options.\n\nIn the speaker production task, participants were presented with the three referents.\nThey were told which object they should refer to.\nThey selected one option from the list of available utterances.\n\nIn the listener interpretation task, participants were presented with the three referents and an utterance.\nThey selected the object that they thought the speaker meant to refer to with that utterance.\n\nIn the salience prior elicitation task, participants again saw all three referents.\nThey were told that the speaker wanted to refer to one of these objects with a word in a language they did not know.\nAgain, they were asked to select the object they thought the speaker wanted to refer to.\nSince this task rids all reasoning about semantic meaning, it is argued to represent a salience baseline of which object is a likely topic of conversation.\n\nWe use the data from the salience prior condition to feed into the pragmatic listener model.\nThe data from the speaker production and the listener interpretation tasks is our training data, i.e., what we want to explain.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
      },
      "outputs": [],
      "source": [
        "##################################################\n## data to fit\n##################################################\n\nsalience_prior = F.normalize(torch.tensor([71,139,30],\n                                          dtype = torch.float32),\n                             p = 1, dim = 0)\n\n# matrix of number of utterance choices for each state\n# (rows: objects, columns: utterances)\nproduction_data = torch.tensor([[9, 135, 0, 0],\n                                [0, 0, 119, 25],\n                                [63, 0, 0, 81]])\n\n# matrix of number of object choices for each ambiguous utterance\n# (rows: utterances, columns: objects)\ninterpretation_data = torch.tensor([[66, 0, 115],   # \"blue\"\n                                    [0, 117, 62]])  # \"square\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "## The RSA model (in PyTorch)\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "Here is an implementation of the vanilla RSA model in PyTorch.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "speaker predictions:\n tensor([[0.0917, 0.9083, 0.0000, 0.0000],\n        [0.0000, 0.0000, 0.2876, 0.7124],\n        [0.1680, 0.0000, 0.0000, 0.8320]])"
        }
      ],
      "source": [
        "##################################################\n## RSA model (forward pass)\n##################################################\n\ndef RSA(alpha, cost_adjectives):\n    costs = torch.tensor([1.0, 0, 1.0, 0]) * cost_adjectives\n    literal_listener   = F.normalize(semantic_meaning, p = 1, dim = 1)\n    pragmatic_speaker  = F.normalize(torch.t(literal_listener)**alpha *\n                                     torch.exp(-alpha * costs), p = 1, dim = 1)\n    pragmatic_listener = F.normalize(torch.t(pragmatic_speaker) * salience_prior, p = 1, dim = 1)\n    return({'speaker': pragmatic_speaker, 'listener': pragmatic_listener})\n\nprint(\"speaker predictions:\\n\", RSA(1, 1.6)['speaker'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "## Parameters to optimize\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "The vanilla RSA model has two free parameters: the optimality parameter $\\alpha$ and the parameter for the cost of utterance, here restricted to a single number for the cost of an adjective (relative to a noun).\nSince we want to optimize the value of these variables, we require PyTorch to compute gradients.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
      },
      "outputs": [],
      "source": [
        "##################################################\n## model parameters to fit\n##################################################\n\nalpha           = torch.tensor(1.0, requires_grad=True) # soft-max parameter\ncost_adjectives = torch.tensor(0.0, requires_grad=True) # differential cost of 'adjectives'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "## Optimization\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "To optimize the model parameters with stochastic gradient descent, we first instantiate an optimizer object, which we tell about the parameter to optimize.\nThe we iterate the training cycle, each time calling the RSA model (feed-forward pass) with the current parameter values, and then computing the (negative) log-likelihood of the data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "#+begin_example\n\n step                     loss           alpha            cost\n  250                 21.74205         2.12154         0.17193\n  500                 16.10578         2.47786         0.15869\n  750                 15.55774         2.58906         0.15650\n 1000                 15.50400         2.62389         0.15597\n 1250                 15.49873         2.63481         0.15582\n 1500                 15.49818         2.63825         0.15577\n 1750                 15.49814         2.63933         0.15576\n 2000                 15.49815         2.63966         0.15575\n 2250                 15.49813         2.63977         0.15575\n 2500                 15.49815         2.63979         0.15575\n 2750                 15.49815         2.63979         0.15575\n 3000                 15.49815         2.63979         0.15575\n 3250                 15.49815         2.63979         0.15575\n 3500                 15.49815         2.63979         0.15575\n 3750                 15.49815         2.63979         0.15575\n 4000                 15.49815         2.63979         0.15575\n#+end_example"
        }
      ],
      "source": [
        "##################################################\n## optimization\n##################################################\n\nopt = torch.optim.SGD([alpha, cost_adjectives], lr = 0.0001)\n\n# output header\nprint('\\n%5s %24s %15s %15s' %\n      (\"step\", \"loss\", \"alpha\", \"cost\") )\n\nfor i in range(4000):\n\n    RSA_prediction      = RSA(alpha, cost_adjectives)\n    speaker_pred        = RSA_prediction['speaker']\n    Multinomial_speaker = torch.distributions.multinomial.Multinomial(144, probs = speaker_pred)\n    logProbs_speaker    = Multinomial_speaker.log_prob(production_data)\n\n    listener_pred          = RSA_prediction['listener']\n    Multinomial_listener_0 = torch.distributions.multinomial.Multinomial(181,probs = listener_pred[0,])\n    logProbs_listener_0    = Multinomial_listener_0.log_prob(interpretation_data[0,])\n    Multinomial_listener_1 = torch.distributions.multinomial.Multinomial(179,probs = listener_pred[3,])\n    logProbs_listener_1    = Multinomial_listener_1.log_prob(interpretation_data[1,])\n\n    loss = -torch.sum(logProbs_speaker) - logProbs_listener_0 - logProbs_listener_1\n\n    loss.backward()\n\n    if (i+1) % 250 == 0:\n        print('%5d %24.5f %15.5f %15.5f' %\n              (i + 1, loss.item(), alpha.item(),\n               cost_adjectives.item()) )\n\n    opt.step()\n    opt.zero_grad()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "> <strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 2.3.1: Comparing model variants </span></strong>\n> 1. We have so far implemented the literal listener as $P_{lit}(s \\mid u) \\propto L_{ij}$. But some RSA models also include the salience prior, which we have so far only used in the pragmatic listener part into the literal listener model. Under this alternative construction the literal listener would be defined as $P_{lit}(s \\mid u) \\propto P_{sal}(s) \\ L_{ij}$. Change the \\`RSA\\` function to implement this alternative definition. (Hint: you only need to add this string somewhere in the code: \\`\\* salience<sub>prior</sub>\\`.) Run the model otherwise as is. Inspect the output of the optimization loop. Use this information to draw conclusions about which of the two model variants is a better predictor of the data.\n> 2. Go back to the original model. We now want to address whether we actually need the cost parameter. Run the original model (w/ a literal listener w/o salience prior information), but optimize only the $\\alpha$ parameter. The cost parameter should be initialized to 0 and stay this way. Fit the model and use the output information to draw conclusions about which model is better: with or without a flexible cost parameter.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "## References\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "Qing, C., & Franke, M. (2015). [Variations on a Bayesian theme: Comparing Bayesian models of referential reasoning](https://michael-franke.github.io/heimseite/Papers/QingFranke_2013_Variations_on_Bayes.pdf). In H. Zeevat, & H. Schmitz (Eds.), Bayesian Natural Language Semantics and Pragmatics (pp. 201–220). Berlin: Springer.\n\n"
      ]
    }
  ],
  "metadata": {
    "org": null,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
