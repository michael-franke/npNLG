{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "> <strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 4.1.1: Explore the training data</span></strong>\n>\n> 1. Remove the noise in the data by changing a single number (parameter value).\n> **Solution:** set &rsquo;scale = 0&rsquo; in the declaration of the distribution object.\n>\n> 2. At what point is the noise in the data so large that there is nothing more to learn?\n> **Solution:** For a value &rsquo;scale = 50&rsquo; there is already rather little signal about $y$ in $x$, but for something like &rsquo;scale>100&rsquo;, the association is almost completely gone.\n\n> <strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 4.1.2: Digest the model, the parameters and their initialization</span></strong>\n>\n> 1. Verify that the functions &rsquo;singleForwardPass&rsquo; and &rsquo;singleForwardPassBatched&rsquo; do the same thing by comparing their predictions for the whole sequence of &rsquo;xObs&rsquo;.\n> I.e., simply call &rsquo;singleForwardPassBatched&rsquo; and compare the output to calls of &rsquo;singleForwardPass&rsquo;.\n> Ideally, produce outputs from &rsquo;singleForwardPass&rsquo; for all elements of &rsquo;xObs&rsquo; by list-comprehension.\n>\n> **Solution:** The output of these are identical (possibly modulo rounding):\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
      },
      "outputs": [],
      "source": [
        "print([singleForwardPass(x).detach().numpy().round(2) for x in xObs])\nprint(singleForwardPassBatched(xObs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        ">\n> 2. Write down this model (forward pass) in mathematical notation. (The mathematical formulation of the model should follow the &rsquo;singleForwardPass&rsquo;).\n>\n> **Solution:** The single forward pass maps a singleton observation $x$ to a singleton predicted value $y$.\n> This proceeds in the following way.\n> First we calculate the hidden layer $h_{1} = U x + b_1$.\n> Then we calculate the next hidden layer as $h_{2} = V_{1} h_{1} + b_2$.\n> Followed by the next hidden layer: $h_{3} = V_{2} h_{2} + b_3$.\n> And finally the output prediction: $y = W h_{3}$.\n>\n> 3. Describe the way parameters are initialized (above) in your own intuitive terms?\n>\n> **Solution:** The parameters of all matrices (think: slope coefficients of each linear transformation) are initialized with random draws from a uniform distribution on the interval $[-1;1]$.\n> All intercepts ($b_{1}$, $b_{2}$, $b_{3}$) are initialized to be zero.\n>\n> 4. Why can we not just set all parameter values to 0 initially?\n>\n> **Solution:** If we set *all* parameters to zero, we cannot train the model, because under the first forward pass *all* parameters contribute exactly equally to the prediction, namely not at all. There is differential training signal telling parameters apart. Nothing can be reasonably updated.\n\n> <strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 4.1.3: Inspecting and interpreting the model fit</span></strong>\n>\n> 1. Inspect the print-outs from the training sequence. Does this output suggest that the training was successful or not?\n>\n> **Solution:** Loss decreases first and then stabilizes, suggesting that learning has reached at least a local minimum (or plateau).\n>\n> 2. What does the plot produced here after training show exactly? Does that picture suggest that the model learned successfully? Do you think that there might even be a sense in which the model &ldquo;[overfitted](https://en.wikipedia.org/wiki/Overfitting)&rdquo; the data?\n>\n> **Solution:** The plot show the original data in blue and the model&rsquo;s predictions in orange. It also shows the &rsquo;ground-truth&rsquo; in green.\n> The closer the orange dots match the $y$-position of the blue dots, the better the model fit.\n> There are indications of &ldquo;overfitting&rdquo; in this case, if we also consider the green dots, the ground-truth model.\n> Sometimes orange dots tend to follow the noisy blue dots away from the original ground-truth model.\n> That is an indication that the model fitted (in part) the noise in the data, not the underlying true shape of the curve.\n>\n> 3. Change the optimizer to vanilla Gradient Descent (&rsquo;SGD&rsquo;), change the training rate to &rsquo;lr=1e-6&rsquo; and the number of training epochs to &rsquo;epochs = 50000&rsquo;.\n> Now, (first re-initialize all parameter values to start training anew) and repeatedly execute the last code cell (probably 4-6 times).\n> Describe what you observe by answering the following questions:\n> (i) is training with &rsquo;SGD&rsquo; more or less efficient than the initial set-up?;\n> (ii) why is it more/less efficient?\n> (iii) if we think of training the model as &ldquo;curve fitting&rdquo;, which parts of the curve are adapted first, which ones later?\n> (iv) explain the difference you described in (iii).\n>\n> **Solution:** (i) GD is less efficient, because (ii) it has a fixed learning rate for all parameters, whereas Adam dynamically (cleverly) adapts the learning rate differently for each parameter.\n> (iv) The more extreme the observation diverges from the model&rsquo;s predictions, the stronger the learning signal. Whence that (iii) first the left- and right-most part of the curve is fitted, then the &ldquo;middle&rdquo; part with the bumps.\n>\n\n"
      ]
    }
  ],
  "metadata": {
    "org": null,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
