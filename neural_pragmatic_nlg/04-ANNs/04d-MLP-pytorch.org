#+title:     Sheet 4.2: Non-linear regression (MLP w/ PyTorch modules)
#+author:    Michael Franke

In this tutorial, we will fit a non-linear regression, implemented as a multi-layer perceptron.
We will see how the use of modules from PyTorch's neural network package `torch.nn` helps us implement the model efficiently.

* Packages & global parameters

We will need to import the `torch` package for the main functionality.
In addition to the previous sheet, In order to have a convenient, we will use PyTorch's `DataLoader` and `Dataset` in order to feed our training data to the model.

#+begin_src jupyter-python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torch.utils.data import Dataset
import numpy as np
import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')
#+end_src

#+RESULTS:


* True model & training data

The "true model" is a constructed non-linear function $y = f(x)$.
Here is its definition and a plot to show what the "ground truth" looks like.

#+begin_src jupyter-python

##################################################
## ground-truth model
##################################################

def goalFun(x):
    return(x**3 - x**2 + 25 * np.sin(2*x))

# create linear sequence (x) and apply goalFun (y)
x = np.linspace(start = -5, stop =5, num = 1000)
y = goalFun(x)

# plot the function
d = pd.DataFrame({'x' : x, 'y' : y})
sns.lineplot(data = d, x = 'x', y = 'y')
plt.show()

#+end_src

#+RESULTS:
[[file:./.ob-jupyter/c817ac06c413126aac2aa6f1e56838120fad5243.png]]

The training data consists of 100 pairs of  $(x,y)$ values.
Each pair is generated by first sampling an $x$ value from a uniform distribution.
For each sampled $x$, we compute the value of the target function $f(x)$ and add Gaussian noise to it.

#+begin_src jupyter-python

##################################################
## generate training data (with noise)
##################################################

nObs = 100 # number of observations

# get noise around y observations
yNormal = torch.distributions.Normal(loc=0.0, scale=10)
yNoise  = yNormal.sample([nObs])

# get observations
xObs = 10*torch.rand([nObs])-5    # uniform from [-5,5]
yObs = xObs**3 - xObs**2 + 25 * torch.sin(2*xObs) + yNoise

# plot the data
d = pd.DataFrame({'xObs' : xObs, 'yObs' : yObs})
sns.scatterplot(data = d, x = 'xObs', y = 'yObs')
plt.show()

#+end_src

#+RESULTS:
[[file:./.ob-jupyter/3ab3a47e68b5a54117e0946341ea7fcea9c70a4a.png]]


* Defining the MLP using PyTorch's built-in modules

As before (sheet 4.1), our model maps a single scalar $x$ onto another scalar $y$.
We use a 3-layer MLP, each hidden layer with dimension 10:


#+begin_src jupyter-python

##################################################
## network dimension parameters
##################################################

nInput  = 1
nHidden = 10
nOutput = 1

#+end_src

#+RESULTS:

PyTorch defines a special-purpose class called `nn.Module` from which pre-defined neural networks or custom-made networks inherit the structure and basic functionality.
Below, we define our feed-forward neural network as a class extending `nn.Module`.
Minimally, we have to define two functions for this to work:

1. the *initialization* function `__init__` which defines which variables (mostly, but not exclusively: parameters) our model has (using `nn.Linear` instantiates a linear layer with all the trainable parameters (weights and biases) implicitly);
2. the *forward pass* which takes the model's input and computes the corresponding prediction given the current parameter values (recall the function `singleForwardPass` from sheet 4.1; PyTorch automatically batches the computation implicitly).

Since PyTorch allows flexibility in how to define neural network modules, we look at two variants below, one explicit and one more concise.
They should implement the exact same model and work the same way eventually.

** More explicit definition NN module

#+begin_src jupyter-python

##################################################
## set up multi-layer perceptron w/ PyTorch
##    -- explicit version --
##################################################

class MLPexplicit(nn.Module):
    '''
    Multi-layer perceptron for non-linear regression.
    '''
    def __init__(self, nInput, nHidden, nOutput):
        super(MLPexplicit, self).__init__()
        self.nInput  = nInput
        self.nHidden = nHidden
        self.nOutput = nOutput
        self.linear1 = nn.Linear(self.nInput, self.nHidden)
        self.linear2 = nn.Linear(self.nHidden, self.nHidden)
        self.linear3 = nn.Linear(self.nHidden, self.nHidden)
        self.linear4 = nn.Linear(self.nHidden, self.nOutput)
        self.ReLU    = nn.ReLU()

    def forward(self, x):
        h1 = self.ReLU(self.linear1(x))
        h2 = self.ReLU(self.linear2(h1))
        h3 = self.ReLU(self.linear3(h2))
        output = self.linear4(h3)
        return(output)

mlpExplicit = MLPexplicit(nInput, nHidden, nOutput)
#+end_src

#+RESULTS:

We can access the current parameter values of this model instance like so:

#+begin_src jupyter-python

for p in mlpExplicit.parameters():
    print(p.detach().numpy().round(4))

#+end_src

#+RESULTS:
#+begin_example
[[ 0.5754]
 [-0.3901]
 [-0.1216]
 [ 0.4368]
 [-0.7784]
 [ 0.15  ]
 [ 0.9548]
 [ 0.76  ]
 [-0.6742]
 [-0.8082]]
[-0.2129 -0.7037 -0.6803  0.8825  0.1179  0.5105 -0.4119  0.8585  0.4957
  0.4588]
[[-1.105e-01  1.049e-01  9.960e-02  1.603e-01 -5.100e-03  1.115e-01
   1.315e-01  3.097e-01  1.370e-01  2.184e-01]
 [ 4.620e-02  2.418e-01  1.997e-01  7.360e-02  1.911e-01  2.698e-01
  -8.780e-02 -2.036e-01 -2.226e-01  3.079e-01]
 [-1.070e-02 -1.925e-01  2.813e-01 -1.675e-01  2.000e-04  2.335e-01
   1.150e-02  2.984e-01  9.800e-03  2.570e-01]
 [-2.690e-01  2.039e-01 -1.051e-01  3.460e-02 -2.071e-01 -2.186e-01
  -2.244e-01  9.820e-02 -1.575e-01  1.749e-01]
 [ 7.890e-02  1.952e-01 -3.112e-01  1.209e-01  3.020e-01  6.110e-02
  -1.955e-01 -2.379e-01  1.938e-01  2.183e-01]
 [-2.540e-02  5.390e-02  1.831e-01  2.810e-01  2.520e-02 -9.470e-02
  -2.660e-01  4.780e-02 -7.660e-02 -5.480e-02]
 [-1.088e-01  1.993e-01 -1.130e-02  8.800e-02  2.383e-01  1.766e-01
   8.600e-02 -1.988e-01 -2.624e-01  5.400e-03]
 [ 1.757e-01  3.100e-02 -2.999e-01 -6.090e-02 -1.035e-01  2.834e-01
   2.010e-01  1.351e-01 -1.018e-01 -2.109e-01]
 [-1.927e-01 -2.414e-01  1.834e-01 -2.416e-01  2.831e-01 -7.700e-03
  -1.306e-01 -2.590e-02  1.442e-01  4.750e-02]
 [ 3.112e-01 -2.927e-01  3.230e-02 -1.119e-01  1.903e-01  1.029e-01
   1.587e-01  2.476e-01  1.617e-01  1.143e-01]]
[-0.2779  0.0276 -0.1886  0.1453 -0.301  -0.0547  0.1382 -0.0395 -0.0626
 -0.2335]
[[-0.0148  0.2349  0.0962 -0.1952  0.2525 -0.2114 -0.1893 -0.1864 -0.1228
   0.1121]
 [-0.1098  0.1322  0.1537  0.3065  0.2219  0.1979 -0.1007  0.0059 -0.2072
   0.0485]
 [-0.2834 -0.2879  0.0674 -0.2409 -0.0165 -0.2056 -0.2703 -0.0451 -0.0155
   0.2517]
 [-0.2729  0.1085  0.2295 -0.3093  0.2927 -0.0862  0.2855  0.1921  0.0792
   0.0331]
 [ 0.1419 -0.281  -0.304  -0.043   0.0542 -0.1305 -0.1999  0.3003  0.0614
   0.0266]
 [ 0.2878 -0.2364  0.0708  0.1285  0.1328 -0.0555  0.1655  0.3043  0.0281
   0.2503]
 [ 0.266  -0.0273 -0.0628 -0.0815 -0.0456  0.2897  0.2182  0.2569  0.2888
  -0.3027]
 [ 0.0553 -0.1376 -0.1899 -0.2387 -0.1187 -0.2584 -0.3043  0.1832 -0.1985
  -0.1817]
 [ 0.244  -0.2198 -0.079   0.2422 -0.1312 -0.1766 -0.2145  0.0222 -0.2277
   0.1826]
 [ 0.1236 -0.2774  0.0451  0.047   0.0147  0.0432 -0.2714  0.1328  0.0576
  -0.3078]]
[-0.0365  0.015   0.169   0.2062  0.1236  0.1869  0.2909  0.1214 -0.175
 -0.073 ]
[[-0.2924  0.2181 -0.0398  0.2618 -0.0346 -0.0977  0.0192  0.2989  0.0297
  -0.0252]]
[0.229]
#+end_example

> <strong><span style="color:#D83D2B;">Exercise 4.2.1: Inspect the model's parameters and their initial values</span></strong>
>
> 0. [Just for yourself.] Make sure that you understand what these parameters are by mapping these onto the parameters of the custom-made model from sheet 4.1. (Hint: the order of the presentation in this print-out is the order in which the components occur in the computation of the forward pass.)
>
> 1. Guess how the weights of the slope matrices are initialized (roughly). Same for the intercept vectors.

** More concise definition of NN module

Here is another, more condensed definition of the same NN model, which uses the `nn.Sequantial` function to neatly chain components, thus defining the model parameters and the forward pass in one swoop.

#+begin_src jupyter-python

##################################################
## set up multi-layer perceptron w/ PyTorch
##    -- condensed version --
##################################################

class MLPcondensed(nn.Module):
    '''
    Multi-layer perceptron for non-linear regression.
    '''
    def __init__(self, nInput, nHidden, nOutput):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(nInput, nHidden),
            nn.ReLU(),
            nn.Linear(nHidden, nHidden),
            nn.ReLU(),
            nn.Linear(nHidden, nHidden),
            nn.ReLU(),
            nn.Linear(nHidden, nOutput)
        )

    def forward(self, x):
        return(self.layers(x))

mlpCondensed = MLPcondensed(nInput, nHidden, nOutput)
#+end_src

#+RESULTS:

Here you can select which one to use in the following.

#+begin_src jupyter-python

# which model to use from here onwards
# model = mlpExplicit
model = mlpCondensed
#+end_src

#+RESULTS:


* Preparing the training data

Data pre-processing is a tedious job, but an integral part of machine learning.
In order to have a clean interface between data processing and modeling, we would ideally like to have a common data format to feed data into any kind of model.
That also makes sharing and reusing data sets much less painful.
For this purpose, PyTorch provides two data primitives: `torch.utils.data.Dataset` and `torch.utils.data.DataLoader`.
The class *Dataset* stores the training data (in a reusable format).
The class *DataLoader* takes a `dataset` object as input and returns an iterable to enable easy access to the training data.

To define a `Dataset` object, we have to specify two key functions:

1. the `__len__` function, which tells subsequent applications how many data points there are; and
2. the `__getitem__` function, which takes an index as input and outputs the data point corresponding to that index.

#+begin_src jupyter-python

##################################################
## representing train data as a Dataset object
##################################################

class nonLinearRegressionData(Dataset):
    '''
    Custom 'Dataset' object for our regression data.
    Must implement these functions: __init__, __len__, and __getitem__.
    '''

    def __init__(self, xObs, yObs):
        self.xObs = torch.reshape(xObs, (len(xObs), 1))
        self.yObs = torch.reshape(yObs, (len(yObs), 1))

    def __len__(self):
        return(len(self.xObs))

    def __getitem__(self, idx):
        return(xObs[idx], yObs[idx])

# instantiate Dataset object for current training data
d = nonLinearRegressionData(xObs, yObs)

# instantiate DataLoader
#    we use the 4 batches of 25 observations each (full data  has 100 observations)
#    we also shuffle the data
train_dataloader = DataLoader(d, batch_size=25 , shuffle=True)

#+end_src

#+RESULTS:

We can test the iterable that we create, just to inspect how the data will be delivered later on:

#+begin_src jupyter-python
for i, data in enumerate(train_dataloader, 0):
    input, target = data
    print("In: ", input)
    print("Out:", target,"\n")
#+end_src

#+RESULTS:
#+begin_example
In:  tensor([-0.7725, -4.4288,  3.1257,  1.9429, -0.3939,  3.1041, -1.5075, -4.4965,
         1.7947,  1.7664, -4.0490, -4.3275,  2.6395, -3.0620,  1.3910, -4.7246,
         3.9388,  1.0197,  0.1928, -1.4034, -2.0079,  4.2810,  4.6774,  0.2034,
         2.0438])
Out: tensor([ -25.5974, -119.1874,   23.0488,  -15.0585,   -8.0903,   28.1141,
          -7.0884, -121.7810,   -3.1409,   -8.0995, -112.0512, -117.8413,
         -21.9975,  -11.7442,    6.3266, -130.9493,   73.9216,   20.5430,
           7.3572,  -16.8534,    3.8495,   41.6217,   70.5874,    8.9183,
         -14.1995])

In:  tensor([ 4.1397e+00,  3.2557e+00, -2.7437e+00,  3.5756e+00,  3.6551e+00,
         4.0927e+00, -3.2258e+00, -4.6758e+00,  2.6409e+00, -4.0434e+00,
         1.8736e+00,  9.8709e-04, -1.0496e+00, -1.5538e+00,  3.2109e+00,
         3.0467e-01,  4.2653e+00,  2.1085e+00,  1.6343e+00, -3.6194e+00,
        -2.3731e+00,  4.2385e+00,  2.2617e+00,  4.9841e+00, -1.7456e+00])
Out: tensor([  84.8706,   28.4794,  -19.2583,   52.4237,   60.5325,   84.6803,
         -43.3596, -154.1162,   -2.5080, -107.6124,   -3.5614,   13.9438,
          -5.4627,   16.9361,   34.5182,   11.9138,   78.0988,  -16.6870,
           3.9416,  -73.3252,    9.9553,   84.3880,  -31.3023,   86.8989,
          14.4626])

In:  tensor([ 0.9750,  0.6785,  4.2401,  4.7220,  4.9017, -2.4038, -0.7460,  2.1952,
        -4.1785,  2.2636, -2.5132, -2.5059,  4.3127,  4.5958, -1.7483, -4.1865,
         1.9364,  2.8476,  1.0666, -1.6643,  0.6199, -1.1807,  4.0197,  3.6992,
         2.9476])
Out: tensor([  11.5008,   28.6247,   89.2554,   96.6934,  101.8484,    2.6467,
         -28.5389,   -8.8933, -124.6718,  -23.1500,  -20.9135,    7.5757,
          78.1339,   68.1331,   -0.1382, -110.8559,  -14.3561,   -7.6898,
          24.7946,   -6.4996,   23.0203,  -24.6201,   84.3156,   47.8314,
         -10.0254])

In:  tensor([ 4.1169, -1.0718,  4.6169,  4.0310, -4.2718,  1.4049, -0.8147,  3.2519,
         4.9448,  0.1882,  2.0649,  4.9499, -2.4810,  2.3281, -2.3521,  2.1797,
        -0.1021, -1.6339, -4.0792, -0.9742,  3.2734, -3.3407,  1.4834,  1.3012,
         3.4758])
Out: tensor([  70.3789,  -25.8226,   81.0158,   71.2713, -111.1335,    4.0316,
          -7.9706,   32.8407,   99.4688,   14.3860,   -4.7079,   79.7343,
          -3.1643,  -19.2599,    1.5282,  -12.1250,   -3.4943,  -14.7574,
        -124.9377,   -6.1291,   24.7051,  -52.5100,   -7.8899,    6.9746,
          43.0192])
#+end_example

* Training the model

We can now train the model similar to how we did this before.
Note that we need to slightly reshape the input data to have the model compute the batched input correctly.

#+begin_src jupyter-python

##################################################
## training the model
##################################################

# Define the loss function and optimizer
loss_function = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
nTrainSteps = 50000

# Run the training loop
for epoch in range(0, nTrainSteps):

  # Set current loss value
  current_loss = 0.0

  # Iterate over the DataLoader for training data
  for i, data in enumerate(train_dataloader, 0):
    # Get inputs
    inputs, targets = data
    # Zero the gradients
    optimizer.zero_grad()
    # Perform forward pass (make sure to supply the input in the right way)
    outputs = model(torch.reshape(inputs, (len(inputs), 1))).squeeze()
    # Compute loss
    loss = loss_function(outputs, targets)
    # Perform backward pass
    loss.backward()
    # Perform optimization
    optimizer.step()
    # Print statistics
    current_loss += loss.item()

  if (epoch + 1) % 2500 == 0:
      print('Loss after epoch %5d: %.3f' %
            (epoch + 1, current_loss))
      current_loss = 0.0

# Process is complete.
print('Training process has finished.')

yPred = np.array([model.forward(torch.tensor([o])).detach().numpy() for o in xObs]).flatten()

# plot the data
d = pd.DataFrame({'xObs' : xObs.detach().numpy(),
                  'yObs' : yObs.detach().numpy(),
                  'yPred': yPred})
dWide = pd.melt(d, id_vars = 'xObs', value_vars= ['yObs', 'yPred'])
sns.scatterplot(data = dWide, x = 'xObs', y = 'value', hue = 'variable', alpha = 0.7)
x = np.linspace(start = -5, stop =5, num = 1000)
y = goalFun(x)
plt.plot(x,y, color='g', alpha = 0.5)
plt.show()

#+end_src

#+RESULTS:
:RESULTS:
#+begin_example
Loss after epoch  2500: 6778.067
Loss after epoch  5000: 4780.144
Loss after epoch  7500: 3622.555
Loss after epoch 10000: 1996.246
Loss after epoch 12500: 926.375
Loss after epoch 15000: 679.498
Loss after epoch 17500: 661.909
Loss after epoch 20000: 652.919
Loss after epoch 22500: 647.045
Loss after epoch 25000: 642.989
Loss after epoch 27500: 640.045
Loss after epoch 30000: 637.475
Loss after epoch 32500: 635.002
Loss after epoch 35000: 632.604
Loss after epoch 37500: 630.329
Loss after epoch 40000: 628.115
Loss after epoch 42500: 625.875
Loss after epoch 45000: 623.710
Loss after epoch 47500: 621.470
Loss after epoch 50000: 619.279
Training process has finished.
#+end_example
[[file:./.ob-jupyter/959e097bd6b3c8d349159ae656722ac4a5bb9fe0.png]]
:END:
> <strong><span style="color:#D83D2B;">Exercise 4.2.2: Explore the model's behavior</span></strong>
>
> 0. [Just for yourself.] Make sure you understand /every line/ in this last code block. Ask if anything is unclear.
>
> 1. Above we used the DataLoader to train in 4 mini-batches. Change it so that there is only one batch containing all the data. Change the `shuffle` parameter so that data is not shuffled. Run the model and check if you observe any notable differences. Explain what your observations. (If you do not see anything, explain why you don't. You might pay attention to the results of training)
