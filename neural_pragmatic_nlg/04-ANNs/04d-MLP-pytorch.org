#+title:     Sheet 3.2: Non-linear regression (MLP w/ PyTorch modules)
#+author:    Michael Franke

In this tutorial, we will fit a non-linear regression, implemented as a multi-layer perceptron.
We will see how the use of modules from PyTorch's neural network package `torch.nn` helps us implement the model in a more efficient way.

* Packages & global parameters

We will need to import the `torch` package for the main functionality.
In order to have a convenient handle, we load the `torch.nn.functional` package into variable `F`.
On top of that, we will use PyTorch's `DataLoader` and `Dataset` in order to feed our training data to the model.

#+begin_src jupyter-python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torch.utils.data import Dataset
import numpy as np
import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')
#+end_src



* True model

The "true model" is a constructed non-linear function $y = f(x)$.
Here is its definition and a plot to show what the "ground truth" looks like.

#+begin_src jupyter-python

##################################################
## ground-truth model
##################################################

def goalFun(x):
    return(x**3 - x**2 + 25 * np.sin(2*x))

# create linear sequence (x) and apply goalFun (y)
x = np.linspace(start = -5, stop =5, num = 1000)
y = goalFun(x)

# plot the function
d = pd.DataFrame({'x' : x, 'y' : y})
sns.lineplot(data = d, x = 'x', y = 'y')
plt.show()

#+end_src

* Training data

The training data consists of 100 pairs of  $(x,y)$ values.
Each pair is generated by first sampling an $x$ value from a uniform distribution.
For each sampled $x$, we compute the value of the target function $f(x)$ and add Gaussian noise to it.

#+begin_src jupyter-python

##################################################
## generate training data (with noise)
##################################################

nObs = 100 # number of observations

# get noise around y observations
yNormal = torch.distributions.Normal(loc=0.0, scale=10)
yNoise  = yNormal.sample([nObs])

# get observations
xObs = 10*torch.rand([nObs])-5    # uniform from [-5,5]
yObs = xObs**3 - xObs**2 + 25 * torch.sin(2*xObs) + yNoise

# plot the data
d = pd.DataFrame({'xObs' : xObs, 'yObs' : yObs})
sns.scatterplot(data = d, x = 'xObs', y = 'yObs')
plt.show()

#+end_src


* Using PyTorch's built-in modules

#+begin_src jupyter-python

##################################################
## network dimension parameters
##################################################

nInput  = 1
nHidden = 10
nOutput = 1

#+end_src

#+begin_src jupyter-python

##################################################
## set up multi-layer perceptron w/ PyTorch
##################################################

class MLP(nn.Module):
    '''
    Multi-layer perceptron for non-linear regression.
    '''
    def __init__(self, nInput, nHidden, nOutput):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(nInput, nHidden),
            nn.ReLU(),
            nn.Linear(nHidden, nHidden),
            nn.ReLU(),
            nn.Linear(nHidden, nHidden),
            nn.ReLU(),
            nn.Linear(nHidden, nOutput)
        )

    def forward(self, x):
        return(self.layers(x))

mlp = MLP(nInput, nHidden, nOutput)

class MLP2(nn.Module):
    '''
    Multi-layer perceptron for non-linear regression.
    '''
    def __init__(self, nInput, nHidden, nOutput):
        super(MLP2, self).__init__()
        self.nInput  = nInput
        self.nHidden = nHidden
        self.nOutput = nOutput
        self.linear1 = nn.Linear(self.nInput, self.nHidden)
        self.linear2 = nn.Linear(self.nHidden, self.nHidden)
        self.linear3 = nn.Linear(self.nHidden, self.nHidden)
        self.linear4 = nn.Linear(self.nHidden, self.nOutput)
        self.ReLU    = nn.ReLU()
        self.linear5 = nn.Linear(1,1)

    def forward(self, x):
        h1 = self.ReLU(self.linear1(x))
        h2 = self.ReLU(self.linear2(h1))
        h3 = self.ReLU(self.linear3(h2))
        output = self.linear5(self.linear4(h3))
        return(output)

mlp2 = MLP2(nInput, nHidden, nOutput)

class LM(nn.Module):
    '''
    Simple linear regression model.
    '''
    def __init__(self):
        super(LM, self).__init__()
        self.linear = nn.Linear(1,1)

    def forward(self, x):
        y = self.linear(x)
        return(y)

model = mlp
# model = mlp2
odel = LM()

# preparing training data
# from: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html
# > Code for processing data samples can get messy and hard to maintain;
# we ideally want our dataset code to be decoupled from our model
# training code for better readability and modularity.
# PyTorch provides two data primitives: torch.utils.data.DataLoader
# and torch.utils.data.Dataset that allow you to use pre-loaded
# datasets as well as your own data. Dataset stores the samples and
# their corresponding labels, and DataLoader wraps an iterable around the
# Dataset to enable easy access to the samples.

class nonLinearRegressionData(Dataset):
    '''
    Custom 'Dataset' object for our regression data.
    Must implement these functions: __init__, __len__, and __getitem__.
    '''

    def __init__(self, xObs, yObs):
        self.xObs = torch.reshape(xObs, (len(xObs), 1))
        self.yObs = torch.reshape(yObs, (len(yObs), 1))

    def __len__(self):
        return(len(self.xObs))

    def __getitem__(self, idx):
        return(xObs[idx], yObs[idx])

d = nonLinearRegressionData(xObs, yObs)

train_dataloader = DataLoader(d, batch_size=100, shuffle=True)

# for i, data in enumerate(train_dataloader, 0):
#     input, target = data
#     print(input, target)
#     output = mlp(torch.reshape(input, (len(input), 1))).squeeze()
#     print(i, input, target, output)

# Define the loss function and optimizer
loss_function = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

model.train()

# Run the training loop
for epoch in range(0, 50000):

  # Print epoch
  # print(f'Starting epoch {epoch+1}')

  # Set current loss value
  current_loss = 0.0

  # Iterate over the DataLoader for training data
  for i, data in enumerate(train_dataloader, 0):

    # Get inputs
    inputs, targets = data

    # Zero the gradients
    optimizer.zero_grad()

    # Perform forward pass
    # outputs = mlp(inputs)
    outputs = model(torch.reshape(inputs, (len(inputs), 1))).squeeze()

    # Compute loss
    loss = loss_function(outputs, targets)

    # Perform backward pass
    loss.backward()

    # Perform optimization
    optimizer.step()

    # Print statistics
    current_loss += loss.item()
    if epoch % 25 == 24:
        print('Loss after epoch %5d: %.3f' %
              (epoch + 1, current_loss))
        current_loss = 0.0

# Process is complete.
print('Training process has finished.')

mlp.eval()

yPred = np.array([model.forward(torch.tensor([o])).detach().numpy() for o in xObs]).flatten()

if True:
    d = pd.DataFrame({'xObs' : xObs.detach().numpy(),
                      'yObs' : yObs.detach().numpy(),
                      'yPred': yPred})
    dWide = pd.melt(d, id_vars = 'xObs', value_vars= ['yObs', 'yPred'])
    sns.scatterplot(data = dWide, x = 'xObs', y = 'value', hue = 'variable')
    plt.show()


#+end_src
