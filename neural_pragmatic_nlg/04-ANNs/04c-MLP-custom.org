#+title:     Sheet 3.1: Non-linear regression (custom MLP)
#+author:    Michael Franke

In this tutorial, we will fit a non-linear regression, implemented as a multi-layer perceptron.
First, we will implement it the hard way: coding the different layers in terms of explicitly spelled out matrix operations.
Then, we will see how the use of modules from PyTorch' neural network package `torch.nn` helps us implement the model in a more efficient way.

* Packages & global parameters

We will need to import the `torch` package for the main functionality.
In order to have a convenient handle, we load the `torch.nn.functional` package into variable `F`.
We use this to refer to the normalization function for tensors: `F.normalize`.
We use the `warnings` package to suppress all warning messages in the notebook.

#+begin_src jupyter-python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torch.utils.data import Dataset
import numpy as np
import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt
# import sys
import warnings
warnings.filterwarnings('ignore')
#+end_src

#+RESULTS:

We will also globally set the float precision to 64 (not strictly speaking necessary):

#+begin_src jupyter-python
torch.set_default_dtype(torch.float64)
#+end_src

#+RESULTS:

* True model

The "true model" is a constructed non-linear function $y = f(x)$.
Here is its definition and a plot to show what the "ground truth" looks like.

#+begin_src jupyter-python

##################################################
## ground-truth model
##################################################

def goalFun(x):
    return(x**3 - x**2 + 25 * np.sin(2*x))

# create linear sequence (x) and apply goalFun (y)
x = np.linspace(start = -5, stop =5, num = 1000)
y = goalFun(x)

# plot the function
d = pd.DataFrame({'x' : x, 'y' : y})
sns.lineplot(data = d, x = 'x', y = 'y')
plt.show()

#+end_src

#+RESULTS:
[[file:./.ob-jupyter/c817ac06c413126aac2aa6f1e56838120fad5243.png]]

* Training data

The training data consists of 100 pairs of  $(x,y)$ values.
Each pair is generated by first sampling an $x$ value from a uniform distribution.
For each sampled $x$, we compute the value of the target function $f(x)$ and add Gaussian noise to it.

#+begin_src jupyter-python

##################################################
## generate training data (with noise)
##################################################

nObs = 100 # number of observations

# get noise around y observations
yNormal = torch.distributions.Normal(loc=0.0, scale=10)
yNoise  = yNormal.sample([nObs])

# get observations
xObs = 10*torch.rand([nObs])-5    # uniform from [-5,5]
yObs = xObs**3 - xObs**2 + 25 * torch.sin(2*xObs) + yNoise

# plot the data
d = pd.DataFrame({'xObs' : xObs, 'yObs' : yObs})
sns.scatterplot(data = d, x = 'xObs', y = 'yObs')
plt.show()

#+end_src

#+RESULTS:


* Hand-coding a multi-layer perceptron

*Caveat:* Training this model is rather slow! That is why we will be happy to see that using the built-in modules from the PyTorch package is more efficient.

Our model maps a single real number $x$ onto another single real number $y$.
We therefore have a neural network which input and output dimension set to 1.
In between $x$ and $y$, we will use three hidden layers $h_{1}, h_{2}, h_{3}$ each with dimension 10:

#+begin_src jupyter-python

##################################################
## network dimension parameters
##################################################

nInput  = 1
nHidden = 10
nOutput = 1

#+end_src

#+RESULTS:

The networks parameters are four matrices and four slopes.
The first embedding is called $U$, the following two hidden-to-hidden transformations are $V_{1}$ and $V_{2}$, the hidden-to-output mapping is given by matrix $W$.

#+begin_src jupyter-python

##################################################
## trainable (matrix & slope) parameters
##################################################

U  = torch.tensor(np.random.rand(nHidden,nInput) * 2 - 1,
                  # dtype=torch.float64,
                  requires_grad=True)
V1 = torch.tensor(np.random.rand(nHidden, nHidden) * 2 - 1,
                  # dtype=torch.float64,
                  requires_grad=True)
V2 = torch.tensor(np.random.rand(nHidden, nHidden) * 2 - 1,
                  # dtype=torch.float64,
                  requires_grad=True)
W  = torch.tensor(np.random.rand(nOutput, nHidden) * 2 - 1,
                  # dtype=torch.float64,
                  requires_grad=True)
b1 = torch.zeros((10,1), requires_grad=True)
b2 = torch.zeros((10,1), requires_grad=True)
b3 = torch.zeros((10,1), requires_grad=True)

#+end_src

#+RESULTS:

Next, we will define the forward pass.

#+begin_src jupyter-python

##################################################
## forward pass
##################################################

activationFun = F.relu # use ReLU fct from PyTorch

# def singleForwardPass(x):
#     h1 = activationFun(U*x)
#     h2 = activationFun(torch.mm(V1,h1))
#     h3 = activationFun(torch.mm(V2,h2))
#     y  = torch.mm(W,h3)
#     return(y[0,0])

def singleForwardPassBatched(xObs):
    xObsBatched = xObs.reshape(100,1,1)       # 100 1x1 matrices
    h1 = activationFun(U @ xObsBatched + b1)  # 100 column vectors
    h2 = activationFun(V1 @ h1 + b1)
    h3 = activationFun(V2 @ h2 + b3)
    y  = W @ h3
    yReshaped = torch.reshape(y,(-1,))
    return(yReshaped)

print(xObs)
#+end_src

#+RESULTS:
#+begin_example
tensor([ 4.8212, -3.6986, -2.2621, -0.6663,  2.5247,  0.8348,  3.3848,  3.3032,
         2.0409,  3.2020, -2.7552, -1.8041,  0.7756, -1.6189, -0.2286,  3.7532,
        -4.5802, -4.2639,  3.0676,  3.1438,  4.6092, -2.7710, -0.9087, -4.7675,
         4.9528, -1.4007, -4.5452, -2.5419,  4.2516,  3.8087,  3.0371,  4.6043,
         2.5801, -2.1410, -4.5425, -3.5939, -0.8856,  3.1534, -2.9107, -1.7105,
        -4.5755,  1.7475,  3.8754, -1.0801, -3.1355, -1.1625,  4.1434,  1.6326,
        -2.1085, -1.9119, -0.6062,  0.4406,  4.9729, -3.6461, -4.2764, -1.0927,
         1.1460, -2.2606,  0.6612,  2.2944, -4.4055, -0.2996,  2.3072,  1.5857,
        -3.9019, -1.4136, -3.5811,  0.1786, -0.4270, -3.4039, -2.9542, -3.1840,
         3.2709, -2.0060, -3.7081, -4.4381,  1.5611,  0.8803,  1.2580, -1.6031,
         0.9409, -2.5717,  3.3835, -2.8618,  0.3153, -0.7607, -3.2067, -2.7112,
        -3.8048,  3.5612,  0.7915, -3.0620, -0.0529, -3.8827,  3.4199,  4.7281,
         4.1319, -2.5712, -3.6042,  2.4338])
#+end_example


It remains to instantiate an optimizer and to run execute the training loop.

Next, we will define the 'forward pass', i.e., a function that computes the predicted $y$ for a given input $x$ based on the current model parameters.

#+begin_src jupyter-python

##################################################
## optimizer & training loop
##################################################

# initialize optimizer: Adam optimizer
loss_function = nn.MSELoss()
opt = torch.optim.Adam([U,V1,V2,W,b1,b2,b3], lr=1e-4)

epochs = 100000
for i in range(epochs+1):
    if (i == 0):
        print("\n")
    yPred = singleForwardPassBatched(xObs)
    loss  = loss_function(yPred, yObs)
    # loss  = torch.mean((yPred - yObs)**2)
    if (i == 0 or i % 2500 == 0):
        print('Iteration: {:5d} | Loss: {:12}'.format(i, loss.detach().numpy().round(0)))
        # print("Iteration: ", i, " Loss: ", loss.detach().numpy())
    loss.backward()
    opt.step()
    opt.zero_grad()

yPred = singleForwardPassBatched(xObs)

# plot the data
if True:
    d = pd.DataFrame({'xObs' : xObs.detach().numpy(),
                      'yObs' : yObs.detach().numpy(),
                      'yPred': yPred.detach().numpy()})
    dWide = pd.melt(d, id_vars = 'xObs', value_vars= ['yObs', 'yPred'])
    sns.scatterplot(data = dWide, x = 'xObs', y = 'value', hue = 'variable')
    plt.show()

#+end_src

#+RESULTS:
:RESULTS:
#+begin_example


Iteration:     0 | Loss:        145.0
Iteration:  2500 | Loss:        120.0
Iteration:  5000 | Loss:        115.0
Iteration:  7500 | Loss:        114.0
Iteration: 10000 | Loss:        113.0
Iteration: 12500 | Loss:        113.0
Iteration: 15000 | Loss:        113.0
Iteration: 17500 | Loss:        113.0
Iteration: 20000 | Loss:        112.0
Iteration: 22500 | Loss:        112.0
Iteration: 25000 | Loss:        112.0
Iteration: 27500 | Loss:        112.0
Iteration: 30000 | Loss:        112.0
Iteration: 32500 | Loss:        112.0
Iteration: 35000 | Loss:        112.0
Iteration: 37500 | Loss:        112.0
Iteration: 40000 | Loss:        112.0
Iteration: 42500 | Loss:        111.0
Iteration: 45000 | Loss:        111.0
Iteration: 47500 | Loss:        111.0
Iteration: 50000 | Loss:        111.0
Iteration: 52500 | Loss:        111.0
Iteration: 55000 | Loss:        110.0
Iteration: 57500 | Loss:        110.0
Iteration: 60000 | Loss:        110.0
Iteration: 62500 | Loss:        110.0
Iteration: 65000 | Loss:        110.0
Iteration: 67500 | Loss:        110.0
Iteration: 70000 | Loss:        110.0
Iteration: 72500 | Loss:        110.0
Iteration: 75000 | Loss:        110.0
Iteration: 77500 | Loss:        110.0
Iteration: 80000 | Loss:        110.0
Iteration: 82500 | Loss:        110.0
Iteration: 85000 | Loss:        110.0
Iteration: 87500 | Loss:        110.0
Iteration: 90000 | Loss:        110.0
Iteration: 92500 | Loss:        110.0
Iteration: 95000 | Loss:        110.0
Iteration: 97500 | Loss:        110.0
Iteration: 100000 | Loss:        110.0
#+end_example
[[file:./.ob-jupyter/0e18af477082aff3b7acee29bc9f841f1f585ff2.png]]
:END:
