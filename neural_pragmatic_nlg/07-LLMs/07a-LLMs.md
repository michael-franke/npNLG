
# Large Language Models

In this session we discuss transformer architectures for *large* language models.
We will learn about the motivation and benefits of transformer models, and how to use pre-trained LLMs through huggingface&rsquo;s &rsquo;transformers&rsquo; package.


## Learning goals for this session

1.  understand limitations RNNs and LSTMs
2.  understand the basics of transformer based architectures
3.  become able to use the &rsquo;transformers&rsquo; package to access pre-trained large-language models


## Slides

Here are the [slides for this session](<https://michael-franke.github.io/npNLG/07-LLMs.pdf>).

