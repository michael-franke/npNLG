{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": "Sheet 1.1: Vanilla RSA\n======================\n\n**Author:** Michael Franke\n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "This is a (numpy-based) Python implementation of a vanilla Rational Speech Act model for a reference game.\n\nThe same model is also covered in [chapter 1 of problang.org](http://www.problang.org/chapters/01-introduction.html).\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "## Packages\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "The \\`numpy\\` package is used here in order to implement the RSA model using vector and matrix computation.\nAdditionally, we use the \\`seaborn\\` package for visualizing the model&rsquo;s predictions.\nThe input for the \\`seaborn\\` plots are Data Frames from the \\`pandas\\` package.\nWe also might need \\`matplotlib\\` to produce (render) the plots.\nFinally, this notebook uses the \\`warnings\\` package to suppress all warning messages (not necessarily best practice in general, but acceptable here, as not all warnings are critical and could just distract).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "## Running example\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "We will use a single running example here.\nThe reference game in question is show below.\n\n![img](../pics/02-reference-game.png)\n\nThere are three objects, all of which could be the speaker&rsquo;s intended referent.\nThe set of utterances consists of the expressions: &rsquo;blue&rsquo;, &rsquo;green&rsquo;, &rsquo;circle&rsquo; and &rsquo;square&rsquo;.\n\nIn python code, we can represent this context as follows:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
      },
      "outputs": [],
      "source": [
        "##################################################\n## defining the context\n##################################################\n\nobject_names    = ['blue_circle', 'green_square', 'blue_square']\nutterance_names = ['blue', 'circle', 'green', 'square']\n\nsemantic_meaning = np.array(\n    # blue circle, green square, blue square\n    [[1, 0, 1],  # blue\n     [1, 0, 0],  # circle\n     [0, 1, 0],  # green\n     [0, 1, 1]]  # square,\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "## Helper functions\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "Two helper functions will come in handy:\none for normalizing vectors and matrices;\nanother for computing soft-max.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
      },
      "outputs": [],
      "source": [
        "##################################################\n## helper functions\n##################################################\n\ndef softmax(x, axis=1):\n    \"\"\"\n    Softmax function in numpy\n    Parameters\n    ----------\n    x: array\n        An array with any dimensionality\n    axis: int\n        The axis along which to apply the softmax\n    Returns\n    -------\n    array\n        Same shape as x\n    \"\"\"\n    e_x = np.exp(x - np.max(x, axis, keepdims=True))\n    return e_x / e_x.sum(axis=axis, keepdims=True)\n\n\ndef normalize(arr, axis=1):\n    \"\"\"\n    Normalize arr along axis\n    \"\"\"\n    return arr / arr.sum(axis, keepdims=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "[[1 2]\n [3 4]\n [5 6]]\n[[0.01587624 0.01587624]\n [0.11731043 0.11731043]\n [0.86681333 0.86681333]]"
        }
      ],
      "source": [
        "##################################################\n## add solutions here for exercise 1.1.1\n##################################################\n\n# softmax(np.hurray([1,1,1,1]), axis=0815)\n\nm = np.array([[1,2], [3,4], [5,6]])\nprint(m)\nprint(softmax(m, axis=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "> <strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 1.1.1: Apply the helper functions</span></strong>\n> 1. Normalize the following matrix to a row-stochastic and a column-stochastic matrix for the matrix: $[[1,2], [3,4]]$.\n> 2. Apply softmax to the following matrix to obtain a row-stochastic and a column-stochastic matrix:  $[[1,2], [3,4], [5,6]]$.\n> 3. Compute the soft-max and the normalized stochastic vector for the vector $[1,2,3]$. (NB: the definitions do *not* expect vector input, so &#x2026; what do you do?)\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "## The model and its parameters\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "The model we want to implement is defined as follows:\n\n\\begin{align*}\nP_{lit}(s \\mid u) & \\propto L(u,s) \\\\\nP_S( u \\mid s) &= \\mathrm{SoftMax}  \\left ( \\alpha \\left ( \\log P_{lit}(s \\mid u)  - \\mathrm{C}(u) \\right ) \\right ) \\\\\nP_L( s \\mid u) & \\propto P_{sal}(s) \\ P_S( u \\mid s)\n\\end{align*}\n\nHere, $L$ is a *lexicon*, which assigns a truth-value (usually 0 or 1) to each pair of utterance and state.\nThe sign $\\propto$ is to be read as &ldquo;proportional to&rdquo; and implies proper normalization of the to-be-defined (conditional) distributions.\nThe parameter $\\alpha$ is the usual &ldquo;optimality&rdquo; or &ldquo;inverse temperature&rdquo; parameter of the soft-max function.\nThe cost function $C$ assigns a real number to each utterance, representing the relative effort or dispreference for that utterance.\n(NB: Since soft-max is only sensitive to (additive) differences, only differences in cost between utterances matter.)\nThe salience prior $P_{sal}$ provides a relative weight of accessibility, salience or *a priori* bias for each object.\n\nThe model&rsquo;s free parameters are: the optimality parameter $\\alpha$, the relative cost $C(u_{\\text{ajd}})$ of using an adjective (rather than a noun, where $C(u_{\\text{noun}})=0$), and the salience prior $P_{sal}$.\n\n> <strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 1.1.2: Paraphrase the definitions</span></strong>\n>\n> Provide a short, intuitive and explanatory paraphrase for each of the three conditional probability distributions that define the RSA model above. I.e., formulate a sentence or two for each, so that a person can understand the purpose or gist of the definition. The less technical jargon you use, the better. The more insightful to a novice, the better.\n\n> <strong><span style=&ldquo;color:#D83D2B;&rdquo;>Solutions for Exercise 1.1.2</span></strong>\n>\n> &#x2026; add your solution here &#x2026;\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
      },
      "outputs": [],
      "source": [
        "##################################################\n## model parameters\n##################################################\n\nalpha              = 1\ncost_adjectives    = 0.1\nsalience_prior_flt = np.array([1,1,1])     # flat\nsalience_prior_emp = np.array([71,139,30]) # empirical"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "## Implementation and visualization\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "#+begin_example\n              blue  circle  green  square        object\nblue_circle   0.31    0.69   0.00    0.00   blue_circle\ngreen_square  0.00    0.00   0.64    0.36  green_square\nblue_square   0.48    0.00   0.00    0.52   blue_square\n        blue_circle  green_square  blue_square utterance\nblue            0.4           0.0          0.6      blue\ncircle          1.0           0.0          0.0    circle\ngreen           0.0           1.0          0.0     green\nsquare          0.0           0.4          0.6    square\n#+end_example"
        },
        {
          "data": {
            "image/png": "",
            "text/plain": "<matplotlib.figure.Figure>"
          },
          "metadata": {
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "",
            "text/plain": "<matplotlib.figure.Figure>"
          },
          "metadata": {
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "##################################################\n## RSA model predictions\n##################################################\n\n\ndef RSA(alpha, cost_adjectives, salience_prior):\n    \"\"\"\n    predictions of the vanilla RSA model for reference game\n    Parameters\n    ----------\n    alpha: float\n        Optimality parameter\n    cost_adjectives: float\n        Differential cost for production of adjectives\n    salience_prior: array\n        Prior over objects\n    Returns\n    -------\n    dictionary\n        Dictionary with keys 'speaker' and 'listener'\n    \"\"\"\n    costs              = np.array([1.0, 0, 1.0, 0]) * cost_adjectives\n    literal_listener   = normalize(semantic_meaning)\n    util_speaker       = np.log(np.transpose(literal_listener)) - costs\n    pragmatic_speaker  = softmax(alpha * util_speaker)\n    pragmatic_listener = normalize(np.transpose(pragmatic_speaker) * salience_prior)\n    return({'speaker': pragmatic_speaker, 'listener': pragmatic_listener})\n\nRSA_predictions = RSA(alpha, cost_adjectives, salience_prior_flt)\n\n##################################################\n## cast model predictions to DataFrames\n##################################################\n\nspeaker  = pd.DataFrame(data = RSA_predictions['speaker'],\n                        index = object_names,\n                        columns = utterance_names)\nspeaker['object'] = speaker.index\nprint(speaker.round(2))\n\nlistener = pd.DataFrame(data    = RSA_predictions['listener'],\n                        index   = utterance_names,\n                        columns = object_names)\nlistener['utterance'] = listener.index\nprint(listener.round(2))\n\n##################################################\n## plotting the results\n##################################################\n\nspeaker_long = speaker.melt(id_vars = \"object\", var_name = \"utterance\",\n                            value_name = \"probability\", ignore_index = False)\nspeaker_plot = sns.FacetGrid(speaker_long, col=\"object\")\nspeaker_plot.map(sns.barplot, \"utterance\", \"probability\")\nplt.show()\n\nlistener_long = listener.melt(id_vars = \"utterance\", var_name = \"object\",\n                              value_name = \"probability\", ignore_index = False)\nlistener_plot = sns.FacetGrid(listener_long, col=\"utterance\")\nlistener_plot.map(sns.barplot, \"object\", \"probability\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "> <strong><span style=&ldquo;color:#D83D2B;&rdquo;>Exercise 1.1.3: Explore the vanilla RSA model</span></strong>\n>\n> 0. [for your own sake only; no written answer required] Go through the whole last code block. Try to understand every single line in the function \\`RSA<sub>predictions</sub>\\`. Ask if anything is unclear. (It is muss less important to understand the details of the subsequent data wrangling and plotting.)\n> 1. Explore what happens if you make the speaker more optimal. Does that also affect the listener&rsquo;s inferences? Why? Is that intuitive?\n> 2. Add another object to the context, namely a red triangle. Add any relevant utterances, their semantics and costs as well. What do you predict will happen to the model&rsquo;s predictions for the &ldquo;old&rdquo; objects and utterances? Test your predictions (= understanding of the model) and report the results.\n> 3. Run the model with different values for the cost parameter \\`cost<sub>adjectives</sub>\\`. Which effect does this have on the speaker prediction? Which effect does that have on the predictions for listener interpretation? Explain these observation in your own non-technical terms (e.g., for an interested outsider).\n> 4. Is there any way to get “blue” to refer to something green? I.e., is it possible (if so: how?; else: why not?) to change the context or model in such a way that $P_{L}(\\text{green square} \\mid \\text{blue}) > 0$, ideally in a way that might also be defensible in that it makes conceptual sense (not just by some technical trick that no reviewer of your work would accept as anything but a hack)?\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "## References\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
      },
      "source": [
        "Frank, M. C., & Goodman, N. D. (2012). Predicting pragmatic reasoning in language games. Science, 336(6084), 998. [http://dx.doi.org/10.1126/science.1218633](http://dx.doi.org/10.1126/science.1218633)\n\nScontras, G., Tessler, M. H., & Franke, M. (2018). [Probabilistic language understanding: An introduction to the Rational Speech Act framework](http://www.problang.org/).\n\n"
      ]
    }
  ],
  "metadata": {
    "org": null,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
